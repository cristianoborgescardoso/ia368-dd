{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\crist\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (4.26.1)\n",
      "Collecting sacrebleu\n",
      "  Using cached sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
      "Requirement already satisfied: torch in c:\\users\\crist\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (1.13.1+cu116)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\crist\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers) (1.24.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\crist\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\crist\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\crist\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\crist\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\crist\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\crist\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers) (3.9.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\crist\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\crist\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\crist\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: portalocker in c:\\users\\crist\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from sacrebleu) (2.7.0)\n",
      "Collecting lxml\n",
      "  Using cached lxml-4.9.2-cp310-cp310-win_amd64.whl (3.8 MB)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in c:\\users\\crist\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\crist\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\crist\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from portalocker->sacrebleu) (305)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\crist\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\crist\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\crist\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\crist\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->transformers) (3.4)\n",
      "Installing collected packages: lxml, sacrebleu\n",
      "Successfully installed lxml-4.9.2 sacrebleu-2.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install transformers sacrebleu torch --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"msmarco_triples.train.tiny.tsv\", sep=\"\\t\", names=[\"query\", \"passage\", \"label\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instancie o modelo T5-base da biblioteca Transformers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02874d7969fc4fbb9a6bd8dc519fa3b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ve/main/spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9c37bae45ee4c6ab6246f8b7c96e371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\crist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6df78f517b754a9f89c525013f6d8f85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0463b26630a14387a6103c8dd5cb492f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-base')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare os dados de treinamento e validação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_data = []\n",
    "val_data = []\n",
    "\n",
    "for i, row in train_df.iterrows():\n",
    "    input_str = f\"document expansion: {row['passage']} </s>\"\n",
    "    target_str = f\"{row['query']} </s>\"\n",
    "    train_data.append((input_str, target_str))\n",
    "\n",
    "for i, row in val_df.iterrows():\n",
    "    input_str = f\"document expansion: {row['passage']} </s>\"\n",
    "    target_str = f\"{row['query']} </s>\"\n",
    "    val_data.append((input_str, target_str))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defina a função de treinamento do modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def train_model(train_data, val_data, tokenizer, model, batch_size=8, num_epochs=5, lr=1e-4, log_steps=100, eval_steps=500):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for i, batch in enumerate(train_dataloader):\n",
    "            input_strs = [data[0] for data in batch]\n",
    "            target_strs = [data[1] for data in batch]\n",
    "            input_ids = tokenizer(input_strs, padding=True, truncation=True, return_tensors='pt').input_ids.to(device)\n",
    "            target_ids = tokenizer(target_strs, padding=True, truncation=True, return_tensors='pt').input_ids.to(device)\n",
    "            labels = target_ids[:, 1:].clone().detach().to(device)\n",
    "            labels[labels == tokenizer.pad_token_id] = -100\n",
    "            outputs = model(input_ids=input_ids.to(device), decoder_input_ids=target_ids[:, :-1].to(device), labels=labels)\n",
    "            loss = outputs.loss\n",
    "            train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            if (i+1) % log_steps == 0:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}, Batch {i+1}/{len(train_dataloader)}, Train Loss: {train_loss/log_steps:.4f}\")\n",
    "                train_loss = 0\n",
    "            if (i+1) % eval_steps == 0:\n",
    "                val_loss = evaluate_model(val_dataloader, tokenizer, model)\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}, Batch {i+1}/{len(train_dataloader)}, Val Loss: {val_loss:.4f}\")\n",
    "                model.train()\n",
    "\n",
    "def evaluate_model(dataloader, tokenizer, model):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        input_strs = [data[0] for data in batch]\n",
    "        target_strs = [data[1] for data in batch]\n",
    "        input_ids = tokenizer(input_strs, padding=True, truncation=True, return_tensors='pt').input_ids.to(device)\n",
    "        target_ids = tokenizer(target_strs, padding=True, truncation=True, return_tensors='pt').input_ids.to(device)\n",
    "        labels = target_ids[:, 1:].clone().detach().to(device)\n",
    "        labels[labels == tokenizer.pad_token_id] = -100\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids.to(device), decoder_input_ids=target_ids[:, :-1].to(device), labels=labels)\n",
    "            loss += outputs.loss.item()\n",
    "    return loss / len(dataloader)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defina as configurações de treinamento e execute o treinamento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\crist\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:226: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Batch 100/1100, Train Loss: 4.9423\n",
      "Epoch 1/5, Batch 200/1100, Train Loss: 4.2545\n",
      "Epoch 1/5, Batch 300/1100, Train Loss: 4.0415\n",
      "Epoch 1/5, Batch 400/1100, Train Loss: 3.9355\n",
      "Epoch 1/5, Batch 500/1100, Train Loss: 3.8471\n",
      "Epoch 1/5, Batch 500/1100, Val Loss: 3.5410\n",
      "Epoch 1/5, Batch 600/1100, Train Loss: 3.7809\n",
      "Epoch 1/5, Batch 700/1100, Train Loss: 3.6957\n",
      "Epoch 1/5, Batch 800/1100, Train Loss: 3.6577\n",
      "Epoch 1/5, Batch 900/1100, Train Loss: 3.7501\n",
      "Epoch 1/5, Batch 1000/1100, Train Loss: 3.6742\n",
      "Epoch 1/5, Batch 1000/1100, Val Loss: 3.4280\n",
      "Epoch 1/5, Batch 1100/1100, Train Loss: 3.7168\n",
      "Epoch 2/5, Batch 100/1100, Train Loss: 3.5575\n",
      "Epoch 2/5, Batch 200/1100, Train Loss: 3.6047\n",
      "Epoch 2/5, Batch 300/1100, Train Loss: 3.6808\n",
      "Epoch 2/5, Batch 400/1100, Train Loss: 3.6334\n",
      "Epoch 2/5, Batch 500/1100, Train Loss: 3.6371\n",
      "Epoch 2/5, Batch 500/1100, Val Loss: 3.3848\n",
      "Epoch 2/5, Batch 600/1100, Train Loss: 3.5662\n",
      "Epoch 2/5, Batch 700/1100, Train Loss: 3.5535\n",
      "Epoch 2/5, Batch 800/1100, Train Loss: 3.5487\n",
      "Epoch 2/5, Batch 900/1100, Train Loss: 3.5734\n",
      "Epoch 2/5, Batch 1000/1100, Train Loss: 3.5523\n",
      "Epoch 2/5, Batch 1000/1100, Val Loss: 3.3541\n",
      "Epoch 2/5, Batch 1100/1100, Train Loss: 3.4743\n",
      "Epoch 3/5, Batch 100/1100, Train Loss: 3.4512\n",
      "Epoch 3/5, Batch 200/1100, Train Loss: 3.4244\n",
      "Epoch 3/5, Batch 300/1100, Train Loss: 3.4433\n",
      "Epoch 3/5, Batch 400/1100, Train Loss: 3.4078\n",
      "Epoch 3/5, Batch 500/1100, Train Loss: 3.5720\n",
      "Epoch 3/5, Batch 500/1100, Val Loss: 3.3351\n",
      "Epoch 3/5, Batch 600/1100, Train Loss: 3.5384\n",
      "Epoch 3/5, Batch 700/1100, Train Loss: 3.4860\n",
      "Epoch 3/5, Batch 800/1100, Train Loss: 3.4980\n",
      "Epoch 3/5, Batch 900/1100, Train Loss: 3.4816\n",
      "Epoch 3/5, Batch 1000/1100, Train Loss: 3.5385\n",
      "Epoch 3/5, Batch 1000/1100, Val Loss: 3.3236\n",
      "Epoch 3/5, Batch 1100/1100, Train Loss: 3.4624\n",
      "Epoch 4/5, Batch 100/1100, Train Loss: 3.5854\n",
      "Epoch 4/5, Batch 200/1100, Train Loss: 3.4382\n",
      "Epoch 4/5, Batch 300/1100, Train Loss: 3.4916\n",
      "Epoch 4/5, Batch 400/1100, Train Loss: 3.5236\n",
      "Epoch 4/5, Batch 500/1100, Train Loss: 3.3401\n",
      "Epoch 4/5, Batch 500/1100, Val Loss: 3.3002\n",
      "Epoch 4/5, Batch 600/1100, Train Loss: 3.3786\n",
      "Epoch 4/5, Batch 700/1100, Train Loss: 3.4201\n",
      "Epoch 4/5, Batch 800/1100, Train Loss: 3.4160\n",
      "Epoch 4/5, Batch 900/1100, Train Loss: 3.4895\n",
      "Epoch 4/5, Batch 1000/1100, Train Loss: 3.4842\n",
      "Epoch 4/5, Batch 1000/1100, Val Loss: 3.2937\n",
      "Epoch 4/5, Batch 1100/1100, Train Loss: 3.4191\n",
      "Epoch 5/5, Batch 100/1100, Train Loss: 3.3019\n",
      "Epoch 5/5, Batch 200/1100, Train Loss: 3.3731\n",
      "Epoch 5/5, Batch 300/1100, Train Loss: 3.3104\n",
      "Epoch 5/5, Batch 400/1100, Train Loss: 3.4468\n",
      "Epoch 5/5, Batch 500/1100, Train Loss: 3.4011\n",
      "Epoch 5/5, Batch 500/1100, Val Loss: 3.2905\n",
      "Epoch 5/5, Batch 600/1100, Train Loss: 3.4559\n",
      "Epoch 5/5, Batch 700/1100, Train Loss: 3.3648\n",
      "Epoch 5/5, Batch 800/1100, Train Loss: 3.4378\n",
      "Epoch 5/5, Batch 900/1100, Train Loss: 3.3930\n",
      "Epoch 5/5, Batch 1000/1100, Train Loss: 3.3514\n",
      "Epoch 5/5, Batch 1000/1100, Val Loss: 3.2889\n",
      "Epoch 5/5, Batch 1100/1100, Train Loss: 3.4667\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "batch_size = 8\n",
    "num_epochs = 5\n",
    "lr = 1e-4\n",
    "log_steps = 100\n",
    "eval_steps = 500\n",
    "model.to(device)\n",
    "# define the path to the saved model\n",
    "model_path = \"./t5_fine_tunned.pt\"\n",
    "# create a new instance of the model\n",
    "model_t5 = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "\n",
    "\n",
    "\n",
    "# Check if the model already exists before loading it\n",
    "if os.path.exists(model_path):\n",
    "    print(f\"Loading existing model from {model_path}\")\n",
    "    # load the saved state dictionary\n",
    "    model_t5.load_state_dict(torch.load(model_path))\n",
    "else:\n",
    "    train_model(train_data, val_data, tokenizer, model, batch_size=batch_size, num_epochs=num_epochs, lr=lr, log_steps=log_steps, eval_steps=eval_steps)\n",
    "    # save the model state dictionary\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"No existing model found at {model_path}. Training new model...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 76\u001b[0m\n\u001b[0;32m     73\u001b[0m log_steps \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m\n\u001b[0;32m     74\u001b[0m eval_steps \u001b[39m=\u001b[39m \u001b[39m500\u001b[39m\n\u001b[1;32m---> 76\u001b[0m train_model(train_data, val_data, tokenizer, model, batch_size\u001b[39m=\u001b[39;49mbatch_size, num_epochs\u001b[39m=\u001b[39;49mnum_epochs, lr\u001b[39m=\u001b[39;49mlr, log_steps\u001b[39m=\u001b[39;49mlog_steps, eval_steps\u001b[39m=\u001b[39;49meval_steps)\n",
      "Cell \u001b[1;32mIn[21], line 41\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(train_data, val_data, tokenizer, model, batch_size, num_epochs, lr, log_steps, eval_steps)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     40\u001b[0m \u001b[39mfor\u001b[39;00m i, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m---> 41\u001b[0m     input_ids \u001b[39m=\u001b[39m batch[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mto(device)\n\u001b[0;32m     42\u001b[0m     attention_mask \u001b[39m=\u001b[39m batch[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     43\u001b[0m     target_ids \u001b[39m=\u001b[39m batch[\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sacrebleu import corpus_bleu\n",
    "from transformers.optimization import AdamW\n",
    "from transformers.optimization import get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(val_data, tokenizer, model, device):\n",
    "    model.eval()\n",
    "    references = []\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_data:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            target_ids = batch['target_ids'].to(device)\n",
    "            output_ids = model.generate(input_ids=input_ids, max_length=50, early_stopping=True)\n",
    "            output_texts = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "            target_texts = tokenizer.batch_decode(target_ids, skip_special_tokens=True)\n",
    "            references.extend(target_texts)\n",
    "            predictions.extend(output_texts)\n",
    "    bleu = corpus_bleu(predictions, [references])\n",
    "    return bleu.score\n",
    "\n",
    "def train_model(train_data, val_data, tokenizer, model, batch_size, num_epochs, lr, log_steps, eval_steps):\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_loader)*num_epochs)\n",
    "    \n",
    "    model.train()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    step = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}\")\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            input_ids = batch[0].to(device)\n",
    "            attention_mask = batch[1].to(device)\n",
    "            target_ids = batch[2].to(device)\n",
    "            decoder_attention_mask = batch[3].to(device)\n",
    "            \n",
    "            model.zero_grad()\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, decoder_input_ids=target_ids[:,:-1], decoder_attention_mask=decoder_attention_mask[:,:-1], labels=target_ids[:,1:])\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            if step % log_steps == 0:\n",
    "                print(f\"Step [{step}/{len(train_loader)*num_epochs}] | Loss: {loss}\")\n",
    "                \n",
    "            if step % eval_steps == 0 and step != 0:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    val_loss, val_bleu = evaluate_model(val_loader, tokenizer, model)\n",
    "                print(f\"Validation Loss: {val_loss} | Validation BLEU: {val_bleu}\")\n",
    "                model.train()\n",
    "            step += 1\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "batch_size = 8\n",
    "num_epochs = 5\n",
    "lr = 1e-4\n",
    "log_steps = 100\n",
    "eval_steps = 500\n",
    "\n",
    "train_model(train_data, val_data, tokenizer, model, batch_size=batch_size, num_epochs=num_epochs, lr=lr, log_steps=log_steps, eval_steps=eval_steps)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
