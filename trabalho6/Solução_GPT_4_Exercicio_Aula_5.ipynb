{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRD6t-zi7Qec"
      },
      "outputs": [],
      "source": [
        "# Baixe o dataset do ms marco tiny\n",
        "#curl -O https://storage.googleapis.com/unicamp-dl/ia368dd_2023s1/msmarco/msmarco_triples.train.tiny.tsv\n",
        "\n",
        "\n",
        "#!pip3 install nmslib\n",
        "#!python -m pip install --upgrade pip\n",
        "#!pip install pyserini\n",
        "#!pip install datasets\n",
        "#!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117\n",
        "#!pip install sacrebleu \n",
        "#!pip install scikit-learn \n",
        "#!pip install torch\n",
        "#!pip install sentencepiece\n",
        "#!pip install transformers\n",
        "#!pip install pandas\n",
        "#!pip install torch\n",
        "#!pip install transformers\n",
        "#!pip install tqdm\n",
        "#!pip install sacrebleu\n",
        "#!pip install scikit-learn\n",
        "#!pip install jupyter notebok\n",
        "#!pip install jsonlines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wd1j56rx7R35"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\windows_apps\\python382\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  warnings.warn(\n",
            "d:\\windows_apps\\python382\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "  8%|▊         | 50/619 [03:17<8:47:09, 55.59s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step: 50, Train Loss: 5.651070511341095, Validation Loss: 0.4829001305759817, Validation BLEU: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 16%|█▌        | 100/619 [06:32<8:01:45, 55.70s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step: 100, Train Loss: 0.7496192157268524, Validation Loss: 0.4414199940536333, Validation BLEU: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 24%|██▍       | 149/619 [06:43<01:39,  4.74it/s]  "
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config, AdamW\n",
        "from tqdm import tqdm\n",
        "import sacrebleu\n",
        "\n",
        "# Atualizar a classe MSMARCODataset\n",
        "class MSMARCODataset(Dataset):\n",
        "    def __init__(self, data_file, tokenizer, max_len):\n",
        "        self.data = pd.read_csv(data_file, delimiter=\"\\t\", header=None, names=[\"query\", \"relevant_passage\", \"non_relevant_passage\"])\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        query = self.data.loc[index, \"query\"]\n",
        "        relevant_passage = self.data.loc[index, \"relevant_passage\"]\n",
        "        tokenized_inputs = self.tokenizer(relevant_passage, return_tensors=\"pt\", max_length=self.max_len, padding=\"max_length\", truncation=True)\n",
        "        tokenized_outputs = self.tokenizer(query, return_tensors=\"pt\", max_length=self.max_len, padding=\"max_length\", truncation=True)\n",
        "        return {\"input_ids\": tokenized_inputs[\"input_ids\"].squeeze(0), \"attention_mask\": tokenized_inputs[\"attention_mask\"].squeeze(0), \"labels\": tokenized_outputs[\"input_ids\"].squeeze(0)}\n",
        "\n",
        "# Load the dataset and split it into training and validation sets\n",
        "data_file = \"msmarco_triples.train.tiny.tsv\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "max_len = 128\n",
        "dataset = MSMARCODataset(data_file, tokenizer, max_len)\n",
        "\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Create DataLoaders for training and validation datasets\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Initialize the model and optimizer\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Train the seq2seq model and validate every X steps  \n",
        "epochs = 2\n",
        "validate_every_x_steps = 50\n",
        "step_count = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_loss_accumulator = 0.0\n",
        "\n",
        "    for batch in tqdm(train_dataloader):\n",
        "        step_count += 1\n",
        "\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        train_loss_accumulator += loss.item()\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Validate the model every X steps\n",
        "        if step_count % validate_every_x_steps == 0:\n",
        "            model.eval()\n",
        "            val_loss_accumulator = 0.0\n",
        "            refs = []\n",
        "            hyps = []\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for val_batch in val_dataloader:\n",
        "                    val_input_ids = val_batch[\"input_ids\"].to(device)\n",
        "                    val_attention_mask = val_batch[\"attention_mask\"].to(device)\n",
        "                    val_labels = val_batch[\"labels\"].to(device)\n",
        "\n",
        "                    val_outputs = model(input_ids=val_input_ids, attention_mask=val_attention_mask, labels=val_labels)\n",
        "                    val_loss = val_outputs.loss\n",
        "                    val_loss_accumulator += val_loss.item()\n",
        "\n",
        "                    generated = model.generate(val_input_ids, attention_mask=val_attention_mask, max_length=max_len)\n",
        "                    hyps.extend(tokenizer.batch_decode(generated, skip_special_tokens=True))\n",
        "                    refs.extend(tokenizer.batch_decode(val_labels, skip_special_tokens=True))\n",
        "\n",
        "            val_loss_avg = val_loss_accumulator / len(val_dataloader)\n",
        "            train_loss_avg = train_loss_accumulator / validate_every_x_steps\n",
        "            bleu = sacrebleu.corpus_bleu(hyps, [refs])\n",
        "            print(f\"Step: {step_count}, Train Loss: {train_loss_avg}, Validation Loss: {val_loss_avg}, Validation BLEU: {bleu.score}\")\n",
        "\n",
        "            train_loss_accumulator = 0.0\n",
        "            model.train()\n",
        "\n",
        "model.save_pretrained(\"doc2query_model\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efX1elpM7eSX"
      },
      "source": [
        "## Gere as consultas expandidas para o TREC-COVID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip install datasets\n",
        "#!pip install pyserini\n",
        "\n",
        "a = trec_covid_corpus[\"corpus\"]\n",
        "b = trec_covid_queries[\"queries\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "import torch\n",
        "\n",
        "# Load the saved model\n",
        "model_path = \"doc2query_model\"\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\", model_max_length=128)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "trec_covid_corpus = load_dataset(\"BeIR/trec-covid\", \"corpus\")\n",
        "trec_covid_queries = load_dataset(\"BeIR/trec-covid\", \"queries\")\n",
        "trec_covid_expanded = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Available keys (splits):\", trec_covid_corpus.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print the first 5 entries\n",
        "for i in range(1000):\n",
        "    print(f\"Entry {i+1}:\")\n",
        "    print(trec_covid_corpus['corpus'][i]['text'])\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#sorted_corpus = sorted(trec_covid_corpus[\"corpus\"], key=lambda x: x[\"_id\"])[:500]\n",
        "# Print the first 5 entries\n",
        "for i in range(500):\n",
        "    print(f\"Entry {i+1}:\")\n",
        "    print(sorted_corpus[i])\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'trec_covid_corpus' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[39mreturn\u001b[39;00m expanded_queries\n\u001b[0;32m     29\u001b[0m \u001b[39m# Get 1000 sorted entries from the dataset\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m sorted_corpus \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(trec_covid_corpus[\u001b[39m\"\u001b[39m\u001b[39mcorpus\u001b[39m\u001b[39m\"\u001b[39m], key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: x[\u001b[39m\"\u001b[39m\u001b[39m_id\u001b[39m\u001b[39m\"\u001b[39m])[:\u001b[39m100\u001b[39m]\n\u001b[0;32m     32\u001b[0m \u001b[39m# Print the first 5 entries\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m5\u001b[39m):\n",
            "\u001b[1;31mNameError\u001b[0m: name 'trec_covid_corpus' is not defined"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "def generate_expanded_queries(document, model, tokenizer, max_len=128, batch_size=500):\n",
        "    # Tokenize the input\n",
        "    tokenized_inputs = tokenizer(document, return_tensors=\"pt\", max_length=max_len, padding=\"max_length\", truncation=True)\n",
        "    input_ids = tokenized_inputs[\"input_ids\"].to(device)\n",
        "    attention_mask = tokenized_inputs[\"attention_mask\"].to(device)\n",
        "\n",
        "    # Split the input into batches\n",
        "    input_ids_batches = torch.split(input_ids, batch_size)\n",
        "    attention_mask_batches = torch.split(attention_mask, batch_size)\n",
        "\n",
        "    # Initialize the output buffer\n",
        "    expanded_queries = []\n",
        "\n",
        "    for input_ids_batch, attention_mask_batch in zip(input_ids_batches, attention_mask_batches):\n",
        "        # Generate the output\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(input_ids=input_ids_batch, attention_mask=attention_mask_batch, max_new_tokens=max_len)\n",
        "        \n",
        "        # Decode the output\n",
        "        for output in outputs:\n",
        "            expanded_query = tokenizer.decode(output, skip_special_tokens=True)\n",
        "            expanded_queries.append(expanded_query)\n",
        "\n",
        "    return expanded_queries\n",
        "\n",
        "# Get 1000 sorted entries from the dataset\n",
        "sorted_corpus = sorted(trec_covid_corpus[\"corpus\"], key=lambda x: x[\"_id\"])[:100]\n",
        "\n",
        "# Print the first 5 entries\n",
        "for i in range(5):\n",
        "    print(f\"Entry {i+1}:\")\n",
        "    print(sorted_corpus[i])\n",
        "    print()\n",
        "\n",
        "for doc in tqdm(sorted_corpus, desc=\"Generating queries\"):\n",
        "    expanded_query = generate_expanded_queries(doc[\"text\"], model, tokenizer, max_len=128, batch_size=500)\n",
        "    trec_covid_expanded.append({\"id\": doc[\"_id\"], \"text\": doc[\"text\"], \"expanded_query\": expanded_query})\n",
        "    print(\"Original Document Text:\")\n",
        "    print(doc[\"text\"])\n",
        "    print(\"Expanded Queries:\")\n",
        "    print(expanded_query)  \n",
        "    print(f\"Generated {len(trec_covid_expanded)} queries\")\n",
        "\n",
        "# Salve as consultas expandidas em um arquivo\n",
        "with open(\"trec_covid_expanded.json\", \"w\") as f:\n",
        "    json.dump(trec_covid_expanded, f)    \n",
        "\n",
        "\n",
        "#for doc in tqdm(trec_covid_corpus[\"corpus\"], desc=\"Generating queries\"):\n",
        "#    expanded_query = generate_expanded_queries(doc[\"text\"], model, tokenizer, max_len=20, batch_size=10000)\n",
        "#    trec_covid_expanded.append({\"id\": doc[\"_id\"], \"text\": doc[\"text\"], \"expanded_query\": expanded_query})"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gerando o Indice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import jsonlines\n",
        "import os\n",
        "\n",
        "workdir = \"trec-covid/\"\n",
        "os.makedirs(workdir, exist_ok=True)\n",
        "\n",
        "json_batch_size = 1 #len(sorted_corpus) // 10\n",
        "j = 0\n",
        "\n",
        "for i in range(0, len(sorted_corpus), json_batch_size):\n",
        "    filename = f\"{workdir}json_{j}.json\"\n",
        "    print(filename)\n",
        "    with jsonlines.open(filename, mode='w') as writer:\n",
        "        for item in sorted_corpus[i:i + json_batch_size]:\n",
        "            writer.write(item)\n",
        "    j += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_BuTFQh7V8f"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pytrec_eval\n",
        "from pyserini.search import SimpleSearcher\n",
        "\n",
        "\n",
        "\n",
        "# Carregue o índice do TREC-COVID\n",
        "searcher = SimpleSearcher(\"beir-v1.0.0-trec-covid-flat\")\n",
        "\n",
        "# BM25 sem expansão\n",
        "def evaluate_bm25_no_expansion(searcher, trec_covid_queries, qrels, k=10):\n",
        "    evaluator = pytrec_eval.RelevanceEvaluator(qrels, {'map_cut', 'ndcg_cut', 'recip_rank'})\n",
        "    topics = {str(topic[\"id\"]): topic[\"query\"] for topic in trec_covid_queries[\"queries\"]}\n",
        "    qrun = {}\n",
        "    for topic_id, query in topics.items():\n",
        "        hits = searcher.search(query, k)\n",
        "        qrun[topic_id] = {hit.docid: idx + 1 for idx, hit in enumerate(hits)}\n",
        "    results = evaluator.evaluate(qrun)\n",
        "    return results['ndcg_cut_10']\n",
        "\n",
        "# BM25 com expansão\n",
        "def evaluate_bm25_expansion(searcher, trec_covid_queries, trec_covid_expanded, qrels, k=10):\n",
        "    evaluator = pytrec_eval.RelevanceEvaluator(qrels, {'map_cut', 'ndcg_cut', 'recip_rank'})\n",
        "    topics = {str(topic[\"id\"]): topic[\"query\"] for topic in trec_covid_queries[\"queries\"]}\n",
        "    expanded_queries = {doc[\"id\"]: doc[\"expanded_query\"] for doc in trec_covid_expanded}\n",
        "    qrun = {}\n",
        "    for topic_id, query in topics.items():\n",
        "        expanded_query = f\"{query} {expanded_queries[topic_id]}\"\n",
        "        hits = searcher.search(expanded_query, k)\n",
        "        qrun[topic_id] = {hit.docid: idx + 1 for idx, hit in enumerate(hits)}\n",
        "    results = evaluator.evaluate(qrun)\n",
        "    return results['ndcg_cut_10']\n",
        "\n",
        "qrels = {str(qrel[\"query_id\"]): {str(qrel[\"doc_id\"]): qrel[\"relevance\"] for qrel in trec_covid_queries[\"qrels\"]} for qrel in trec_covid_queries[\"queries\"]}\n",
        "\n",
        "bm25_no_expansion_ndcg = evaluate_bm25_no_expansion(searcher, trec_covid_queries, qrels)\n",
        "bm25_expansion_ndcg = evaluate_bm25_expansion(searcher, trec_covid_queries, trec_covid_expanded, qrels)\n",
        "\n",
        "print(f\"nDCG@10 do BM25 sem expansão: {bm25_no_expansion_ndcg}\")\n",
        "print(f\"nDCG@10 do BM25 com expansão: {bm25_expansion_ndcg}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
