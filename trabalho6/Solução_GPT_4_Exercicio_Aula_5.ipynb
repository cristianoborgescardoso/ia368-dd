{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRD6t-zi7Qec"
      },
      "outputs": [],
      "source": [
        "# Baixe o dataset do ms marco tiny\n",
        "#curl -O https://storage.googleapis.com/unicamp-dl/ia368dd_2023s1/msmarco/msmarco_triples.train.tiny.tsv\n",
        "\n",
        "\n",
        "#!pip3 install nmslib\n",
        "#!python -m pip install --upgrade pip\n",
        "#!pip install pyserini\n",
        "#!pip install datasets\n",
        "#!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117\n",
        "#!pip install sacrebleu \n",
        "#!pip install scikit-learn \n",
        "#!pip install torch\n",
        "#!pip install sentencepiece\n",
        "#!pip install transformers\n",
        "#!pip install pandas\n",
        "#!pip install torch\n",
        "#!pip install transformers\n",
        "#!pip install tqdm\n",
        "#!pip install sacrebleu\n",
        "#!pip install scikit-learn\n",
        "#!pip install jupyter notebok\n",
        "#!pip install jsonlines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wd1j56rx7R35"
      },
      "outputs": [],
      "source": [
        "!export PYTORCH_CUDA_ALLOC_CONF=garbage_collection_threshold:0.6\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config, AdamW\n",
        "from tqdm import tqdm\n",
        "import sacrebleu\n",
        "\n",
        "# Atualizar a classe MSMARCODataset\n",
        "class MSMARCODataset(Dataset):\n",
        "    def __init__(self, data_file, tokenizer, max_len):\n",
        "        self.data = pd.read_csv(data_file, delimiter=\"\\t\", header=None, names=[\"query\", \"relevant_passage\", \"non_relevant_passage\"])\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        query = self.data.loc[index, \"query\"]\n",
        "        relevant_passage = self.data.loc[index, \"relevant_passage\"]\n",
        "        tokenized_inputs = self.tokenizer(relevant_passage, return_tensors=\"pt\", max_length=self.max_len, padding=\"max_length\", truncation=True)\n",
        "        tokenized_outputs = self.tokenizer(query, return_tensors=\"pt\", max_length=self.max_len, padding=\"max_length\", truncation=True)\n",
        "        return {\"input_ids\": tokenized_inputs[\"input_ids\"].squeeze(0), \"attention_mask\": tokenized_inputs[\"attention_mask\"].squeeze(0), \"labels\": tokenized_outputs[\"input_ids\"].squeeze(0)}\n",
        "\n",
        "# Load the dataset and split it into training and validation sets\n",
        "data_file = \"msmarco_triples.train.tiny.tsv\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "max_len = 128\n",
        "dataset = MSMARCODataset(data_file, tokenizer, max_len)\n",
        "\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Create DataLoaders for training and validation datasets\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Initialize the model and optimizer\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Train the seq2seq model and validate every X steps  \n",
        "epochs = 2\n",
        "validate_every_x_steps = 50\n",
        "step_count = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_loss_accumulator = 0.0\n",
        "\n",
        "    for batch in tqdm(train_dataloader):\n",
        "        step_count += 1\n",
        "\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        train_loss_accumulator += loss.item()\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Validate the model every X steps\n",
        "        if step_count % validate_every_x_steps == 0:\n",
        "            model.eval()\n",
        "            val_loss_accumulator = 0.0\n",
        "            refs = []\n",
        "            hyps = []\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for val_batch in val_dataloader:\n",
        "                    val_input_ids = val_batch[\"input_ids\"].to(device)\n",
        "                    val_attention_mask = val_batch[\"attention_mask\"].to(device)\n",
        "                    val_labels = val_batch[\"labels\"].to(device)\n",
        "\n",
        "                    val_outputs = model(input_ids=val_input_ids, attention_mask=val_attention_mask, labels=val_labels)\n",
        "                    val_loss = val_outputs.loss\n",
        "                    val_loss_accumulator += val_loss.item()\n",
        "\n",
        "                    generated = model.generate(val_input_ids, attention_mask=val_attention_mask, max_length=max_len)\n",
        "                    hyps.extend(tokenizer.batch_decode(generated, skip_special_tokens=True))\n",
        "                    refs.extend(tokenizer.batch_decode(val_labels, skip_special_tokens=True))\n",
        "\n",
        "            val_loss_avg = val_loss_accumulator / len(val_dataloader)\n",
        "            train_loss_avg = train_loss_accumulator / validate_every_x_steps\n",
        "            bleu = sacrebleu.corpus_bleu(hyps, [refs])\n",
        "            print(f\"Step: {step_count}, Train Loss: {train_loss_avg}, Validation Loss: {val_loss_avg}, Validation BLEU: {bleu.score}\")\n",
        "\n",
        "            train_loss_accumulator = 0.0\n",
        "            model.train()\n",
        "\n",
        "model.save_pretrained(\"doc2query_model\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efX1elpM7eSX"
      },
      "source": [
        "## Gere as consultas expandidas para o TREC-COVID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip install datasets\n",
        "#!pip install pyserini\n",
        "\n",
        "a = trec_covid_corpus[\"corpus\"]\n",
        "b = trec_covid_queries[\"queries\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "import torch\n",
        "\n",
        "# Load the saved model\n",
        "model_path = \"doc2query_model\"\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\", model_max_length=128)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "trec_covid_corpus = load_dataset(\"BeIR/trec-covid\", \"corpus\")\n",
        "trec_covid_queries = load_dataset(\"BeIR/trec-covid\", \"queries\")\n",
        "trec_covid_expanded = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Available keys (splits):\", trec_covid_corpus.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print the first 5 entries\n",
        "for i in range(1000):\n",
        "    print(f\"Entry {i+1}:\")\n",
        "    print(trec_covid_corpus['corpus'][i])\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#sorted_corpus = sorted(trec_covid_corpus[\"corpus\"], key=lambda x: x[\"_id\"])[:500]\n",
        "# Print the first 5 entries\n",
        "for i in range(500):\n",
        "    print(f\"Entry {i+1}:\")\n",
        "    print(trec_covid_queries['queries'][i])\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "def generate_expanded_queries(document, model, tokenizer, max_len=128, batch_size=500):\n",
        "    # Tokenize the input\n",
        "    tokenized_inputs = tokenizer(document, return_tensors=\"pt\", max_length=max_len, padding=\"max_length\", truncation=True)\n",
        "    input_ids = tokenized_inputs[\"input_ids\"].to(device)\n",
        "    attention_mask = tokenized_inputs[\"attention_mask\"].to(device)\n",
        "\n",
        "    # Split the input into batches\n",
        "    input_ids_batches = torch.split(input_ids, batch_size)\n",
        "    attention_mask_batches = torch.split(attention_mask, batch_size)\n",
        "\n",
        "    # Initialize the output buffer\n",
        "    expanded_queries = []\n",
        "\n",
        "    for input_ids_batch, attention_mask_batch in zip(input_ids_batches, attention_mask_batches):\n",
        "        # Generate the output\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(input_ids=input_ids_batch, attention_mask=attention_mask_batch, max_new_tokens=max_len)\n",
        "        \n",
        "        # Decode the output\n",
        "        for output in outputs:\n",
        "            expanded_query = tokenizer.decode(output, skip_special_tokens=True)\n",
        "            expanded_queries.append(expanded_query)\n",
        "\n",
        "    return expanded_queries\n",
        "\n",
        "# Get 1000 sorted entries from the dataset\n",
        "#sorted_corpus = sorted(trec_covid_corpus[\"corpus\"], key=lambda x: x[\"_id\"])[:100]\n",
        "sorted_corpus = trec_covid_corpus[\"corpus\"]\n",
        "\n",
        "# Print the first 5 entries\n",
        "for i in range(5):\n",
        "    print(f\"Entry {i+1}:\")\n",
        "    print(sorted_corpus[i])\n",
        "    print()\n",
        "\n",
        "for doc in tqdm(sorted_corpus, desc=\"Generating queries\"):\n",
        "    expanded_query = generate_expanded_queries(doc[\"text\"], model, tokenizer, max_len=128, batch_size=500)\n",
        "    trec_covid_expanded.append({\"id\": doc[\"_id\"], \"text\": doc[\"text\"], \"expanded_query\": expanded_query})\n",
        "    #print(\"Original Document Text:\")\n",
        "    #print(doc[\"text\"])\n",
        "    #print(\"Expanded Queries:\")\n",
        "    #print(expanded_query)  \n",
        "    #print(f\"Generated {len(trec_covid_expanded)} queries\")\n",
        "\n",
        "# Salve as consultas expandidas em um arquivo\n",
        "with open(\"trec_covid_expanded.json\", \"w\") as f:\n",
        "    json.dump(trec_covid_expanded, f)    \n",
        "\n",
        "\n",
        "#for doc in tqdm(trec_covid_corpus[\"corpus\"], desc=\"Generating queries\"):\n",
        "#    expanded_query = generate_expanded_queries(doc[\"text\"], model, tokenizer, max_len=20, batch_size=10000)\n",
        "#    trec_covid_expanded.append({\"id\": doc[\"_id\"], \"text\": doc[\"text\"], \"expanded_query\": expanded_query})"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gerando o Indice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import jsonlines\n",
        "import os\n",
        "\n",
        "workdir = \"trec-covid/\"\n",
        "os.makedirs(workdir, exist_ok=True)\n",
        "\n",
        "json_batch_size = 1 #len(sorted_corpus) // 10\n",
        "j = 0\n",
        "\n",
        "for i in range(0, len(sorted_corpus), json_batch_size):\n",
        "    filename = f\"{workdir}json_{j}.json\"\n",
        "    print(filename)\n",
        "    with jsonlines.open(filename, mode='w') as writer:\n",
        "        for item in sorted_corpus[i:i + json_batch_size]:\n",
        "            writer.write(item)\n",
        "    j += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_BuTFQh7V8f"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pytrec_eval\n",
        "from pyserini.search import SimpleSearcher\n",
        "\n",
        "\n",
        "\n",
        "# Carregue o índice do TREC-COVID\n",
        "searcher = SimpleSearcher(\"beir-v1.0.0-trec-covid-flat\")\n",
        "\n",
        "# BM25 sem expansão\n",
        "def evaluate_bm25_no_expansion(searcher, trec_covid_queries, qrels, k=10):\n",
        "    evaluator = pytrec_eval.RelevanceEvaluator(qrels, {'map_cut', 'ndcg_cut', 'recip_rank'})\n",
        "    topics = {str(topic[\"id\"]): topic[\"query\"] for topic in trec_covid_queries[\"queries\"]}\n",
        "    qrun = {}\n",
        "    for topic_id, query in topics.items():\n",
        "        hits = searcher.search(query, k)\n",
        "        qrun[topic_id] = {hit.docid: idx + 1 for idx, hit in enumerate(hits)}\n",
        "    results = evaluator.evaluate(qrun)\n",
        "    return results['ndcg_cut_10']\n",
        "\n",
        "# BM25 com expansão\n",
        "def evaluate_bm25_expansion(searcher, trec_covid_queries, trec_covid_expanded, qrels, k=10):\n",
        "    evaluator = pytrec_eval.RelevanceEvaluator(qrels, {'map_cut', 'ndcg_cut', 'recip_rank'})\n",
        "    topics = {str(topic[\"id\"]): topic[\"query\"] for topic in trec_covid_queries[\"queries\"]}\n",
        "    expanded_queries = {doc[\"id\"]: doc[\"expanded_query\"] for doc in trec_covid_expanded}\n",
        "    qrun = {}\n",
        "    for topic_id, query in topics.items():\n",
        "        expanded_query = f\"{query} {expanded_queries[topic_id]}\"\n",
        "        hits = searcher.search(expanded_query, k)\n",
        "        qrun[topic_id] = {hit.docid: idx + 1 for idx, hit in enumerate(hits)}\n",
        "    results = evaluator.evaluate(qrun)\n",
        "    return results['ndcg_cut_10']\n",
        "\n",
        "qrels = {str(qrel[\"query_id\"]): {str(qrel[\"doc_id\"]): qrel[\"relevance\"] for qrel in trec_covid_queries[\"qrels\"]} for qrel in trec_covid_queries[\"queries\"]}\n",
        "\n",
        "bm25_no_expansion_ndcg = evaluate_bm25_no_expansion(searcher, trec_covid_queries, qrels)\n",
        "bm25_expansion_ndcg = evaluate_bm25_expansion(searcher, trec_covid_queries, trec_covid_expanded, qrels)\n",
        "\n",
        "print(f\"nDCG@10 do BM25 sem expansão: {bm25_no_expansion_ndcg}\")\n",
        "print(f\"nDCG@10 do BM25 com expansão: {bm25_expansion_ndcg}\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load the data for evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found cached dataset trec-covid (C:/Users/crist/.cache/huggingface/datasets/BeIR___trec-covid/corpus/0.0.0/093f1fe2ffa7a9c72fa48239c8f279b51d6b171abd77737c7fd1406125307599)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8647ec7037904c1586ef12542797e4e4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found cached dataset trec-covid (C:/Users/crist/.cache/huggingface/datasets/BeIR___trec-covid/queries/0.0.0/093f1fe2ffa7a9c72fa48239c8f279b51d6b171abd77737c7fd1406125307599)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4395dee3283e4a00ba62aeab0925e3b9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found cached dataset csv (C:/Users/crist/.cache/huggingface/datasets/BeIR___csv/BeIR--trec-covid-qrels-1766e3af5b0b856a/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "67a39920224b4e7bb5300c96f480eb0c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load the data for evaluation\n",
        "\n",
        "from datasets import load_dataset\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "trec_covid_corpus = load_dataset(\"BeIR/trec-covid\", \"corpus\")\n",
        "trec_covid_queries = load_dataset(\"BeIR/trec-covid\", \"queries\")\n",
        "# Convert datasets to dictionaries\n",
        "trec_covid_corpus = trec_covid_corpus[\"corpus\"].to_dict()\n",
        "trec_covid_queries = trec_covid_queries[\"queries\"].to_dict()\n",
        "\n",
        "# Load the trec-covid-qrels dataset\n",
        "trec_covid_qrels = load_dataset(\"BeIR/trec-covid-qrels\")\n",
        "\n",
        "# Create a DataFrame to store the qrels data\n",
        "qrels = pd.DataFrame()\n",
        "qrels[\"query_id\"] = trec_covid_qrels['test'][\"query-id\"]\n",
        "qrels[\"corpus_id\"] = trec_covid_qrels['test'][\"corpus-id\"]\n",
        "qrels[\"score\"] = trec_covid_qrels['test'][\"score\"]\n",
        "\n",
        "# Create a dictionary from qrels data\n",
        "qrels_dict = {}\n",
        "for query_id, corpus_id, score in zip(qrels[\"query_id\"], qrels[\"corpus_id\"], qrels[\"score\"]):\n",
        "    query_id = str(query_id)\n",
        "    corpus_id = corpus_id.strip()\n",
        "    score = int(score)\n",
        "    if query_id not in qrels_dict:\n",
        "        qrels_dict[query_id] = {}\n",
        "    qrels_dict[query_id][corpus_id] = score\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Generating the indexes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index the original dataset\n",
            "WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\n",
            "2023-04-18 23:20:11,857 INFO  [main] index.IndexCollection (IndexCollection.java:380) - Setting log level to INFO\n",
            "2023-04-18 23:20:11,859 INFO  [main] index.IndexCollection (IndexCollection.java:383) - Starting indexer...\n",
            "2023-04-18 23:20:11,859 INFO  [main] index.IndexCollection (IndexCollection.java:384) - ============ Loading Parameters ============\n",
            "2023-04-18 23:20:11,859 INFO  [main] index.IndexCollection (IndexCollection.java:385) - DocumentCollection path: trec_covid_original_data\n",
            "2023-04-18 23:20:11,859 INFO  [main] index.IndexCollection (IndexCollection.java:386) - CollectionClass: JsonCollection\n",
            "2023-04-18 23:20:11,859 INFO  [main] index.IndexCollection (IndexCollection.java:387) - Generator: DefaultLuceneDocumentGenerator\n",
            "2023-04-18 23:20:11,860 INFO  [main] index.IndexCollection (IndexCollection.java:388) - Threads: 12\n",
            "2023-04-18 23:20:11,860 INFO  [main] index.IndexCollection (IndexCollection.java:389) - Language: en\n",
            "2023-04-18 23:20:11,860 INFO  [main] index.IndexCollection (IndexCollection.java:390) - Stemmer: porter\n",
            "2023-04-18 23:20:11,861 INFO  [main] index.IndexCollection (IndexCollection.java:391) - Keep stopwords? false\n",
            "2023-04-18 23:20:11,861 INFO  [main] index.IndexCollection (IndexCollection.java:392) - Stopwords: null\n",
            "2023-04-18 23:20:11,861 INFO  [main] index.IndexCollection (IndexCollection.java:393) - Store positions? true\n",
            "2023-04-18 23:20:11,861 INFO  [main] index.IndexCollection (IndexCollection.java:394) - Store docvectors? true\n",
            "2023-04-18 23:20:11,861 INFO  [main] index.IndexCollection (IndexCollection.java:395) - Store document \"contents\" field? false\n",
            "2023-04-18 23:20:11,861 INFO  [main] index.IndexCollection (IndexCollection.java:396) - Store document \"raw\" field? true\n",
            "2023-04-18 23:20:11,861 INFO  [main] index.IndexCollection (IndexCollection.java:397) - Additional fields to index: []\n",
            "2023-04-18 23:20:11,862 INFO  [main] index.IndexCollection (IndexCollection.java:398) - Optimize (merge segments)? false\n",
            "2023-04-18 23:20:11,862 INFO  [main] index.IndexCollection (IndexCollection.java:399) - Whitelist: null\n",
            "2023-04-18 23:20:11,862 INFO  [main] index.IndexCollection (IndexCollection.java:400) - Pretokenized?: false\n",
            "2023-04-18 23:20:11,862 INFO  [main] index.IndexCollection (IndexCollection.java:401) - Index path: trec_covid_original_index\n",
            "2023-04-18 23:20:11,864 INFO  [main] index.IndexCollection (IndexCollection.java:481) - ============ Indexing Collection ============\n",
            "2023-04-18 23:20:11,872 INFO  [main] index.IndexCollection (IndexCollection.java:468) - Using DefaultEnglishAnalyzer\n",
            "2023-04-18 23:20:11,872 INFO  [main] index.IndexCollection (IndexCollection.java:469) - Stemmer: porter\n",
            "2023-04-18 23:20:11,873 INFO  [main] index.IndexCollection (IndexCollection.java:470) - Keep stopwords? false\n",
            "2023-04-18 23:20:11,873 INFO  [main] index.IndexCollection (IndexCollection.java:471) - Stopwords file: null\n",
            "2023-04-18 23:20:11,969 INFO  [main] index.IndexCollection (IndexCollection.java:510) - Thread pool with 12 threads initialized.\n",
            "2023-04-18 23:20:11,971 INFO  [main] index.IndexCollection (IndexCollection.java:512) - Initializing collection in trec_covid_original_data\n",
            "2023-04-18 23:20:11,971 INFO  [main] index.IndexCollection (IndexCollection.java:521) - 1 file found\n",
            "2023-04-18 23:20:11,971 INFO  [main] index.IndexCollection (IndexCollection.java:522) - Starting to index...\n",
            "2023-04-18 23:20:12,004 ERROR [pool-2-thread-1] index.IndexCollection$LocalIndexerThread (IndexCollection.java:348) - pool-2-thread-1: Unexpected Exception:\n",
            "java.lang.RuntimeException: JSON document has no \"contents\" field!\n",
            "\tat io.anserini.collection.JsonCollection$Document.contents(JsonCollection.java:218) ~[anserini-0.21.0-fatjar.jar:?]\n",
            "\tat io.anserini.index.generator.DefaultLuceneDocumentGenerator.createDocument(DefaultLuceneDocumentGenerator.java:63) ~[anserini-0.21.0-fatjar.jar:?]\n",
            "\tat io.anserini.index.IndexCollection$LocalIndexerThread.run(IndexCollection.java:295) [anserini-0.21.0-fatjar.jar:?]\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]\n",
            "\tat java.lang.Thread.run(Thread.java:834) [?:?]\n",
            "2023-04-18 23:20:12,036 INFO  [main] index.IndexCollection (IndexCollection.java:578) - Indexing Complete! 0 documents indexed\n",
            "2023-04-18 23:20:12,036 INFO  [main] index.IndexCollection (IndexCollection.java:579) - ============ Final Counter Values ============\n",
            "2023-04-18 23:20:12,036 INFO  [main] index.IndexCollection (IndexCollection.java:580) - indexed:                0\n",
            "2023-04-18 23:20:12,036 INFO  [main] index.IndexCollection (IndexCollection.java:581) - unindexable:            0\n",
            "2023-04-18 23:20:12,037 INFO  [main] index.IndexCollection (IndexCollection.java:582) - empty:                  0\n",
            "2023-04-18 23:20:12,037 INFO  [main] index.IndexCollection (IndexCollection.java:583) - skipped:                0\n",
            "2023-04-18 23:20:12,037 INFO  [main] index.IndexCollection (IndexCollection.java:584) - errors:                 0\n",
            "2023-04-18 23:20:12,041 INFO  [main] index.IndexCollection (IndexCollection.java:587) - Total 0 documents indexed in 00:00:00\n",
            "\n",
            "\n",
            "Index the expanded dataset\n",
            "WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\n",
            "2023-04-18 23:20:13,921 INFO  [main] index.IndexCollection (IndexCollection.java:380) - Setting log level to INFO\n",
            "2023-04-18 23:20:13,924 INFO  [main] index.IndexCollection (IndexCollection.java:383) - Starting indexer...\n",
            "2023-04-18 23:20:13,924 INFO  [main] index.IndexCollection (IndexCollection.java:384) - ============ Loading Parameters ============\n",
            "2023-04-18 23:20:13,924 INFO  [main] index.IndexCollection (IndexCollection.java:385) - DocumentCollection path: trec_covid_expanded_data\n",
            "2023-04-18 23:20:13,924 INFO  [main] index.IndexCollection (IndexCollection.java:386) - CollectionClass: JsonCollection\n",
            "2023-04-18 23:20:13,924 INFO  [main] index.IndexCollection (IndexCollection.java:387) - Generator: DefaultLuceneDocumentGenerator\n",
            "2023-04-18 23:20:13,924 INFO  [main] index.IndexCollection (IndexCollection.java:388) - Threads: 12\n",
            "2023-04-18 23:20:13,925 INFO  [main] index.IndexCollection (IndexCollection.java:389) - Language: en\n",
            "2023-04-18 23:20:13,925 INFO  [main] index.IndexCollection (IndexCollection.java:390) - Stemmer: porter\n",
            "2023-04-18 23:20:13,925 INFO  [main] index.IndexCollection (IndexCollection.java:391) - Keep stopwords? false\n",
            "2023-04-18 23:20:13,925 INFO  [main] index.IndexCollection (IndexCollection.java:392) - Stopwords: null\n",
            "2023-04-18 23:20:13,925 INFO  [main] index.IndexCollection (IndexCollection.java:393) - Store positions? true\n",
            "2023-04-18 23:20:13,925 INFO  [main] index.IndexCollection (IndexCollection.java:394) - Store docvectors? true\n",
            "2023-04-18 23:20:13,925 INFO  [main] index.IndexCollection (IndexCollection.java:395) - Store document \"contents\" field? false\n",
            "2023-04-18 23:20:13,926 INFO  [main] index.IndexCollection (IndexCollection.java:396) - Store document \"raw\" field? true\n",
            "2023-04-18 23:20:13,926 INFO  [main] index.IndexCollection (IndexCollection.java:397) - Additional fields to index: []\n",
            "2023-04-18 23:20:13,926 INFO  [main] index.IndexCollection (IndexCollection.java:398) - Optimize (merge segments)? false\n",
            "2023-04-18 23:20:13,926 INFO  [main] index.IndexCollection (IndexCollection.java:399) - Whitelist: null\n",
            "2023-04-18 23:20:13,926 INFO  [main] index.IndexCollection (IndexCollection.java:400) - Pretokenized?: false\n",
            "2023-04-18 23:20:13,926 INFO  [main] index.IndexCollection (IndexCollection.java:401) - Index path: trec_covid_expanded_index\n",
            "2023-04-18 23:20:13,929 INFO  [main] index.IndexCollection (IndexCollection.java:481) - ============ Indexing Collection ============\n",
            "2023-04-18 23:20:13,939 INFO  [main] index.IndexCollection (IndexCollection.java:468) - Using DefaultEnglishAnalyzer\n",
            "2023-04-18 23:20:13,939 INFO  [main] index.IndexCollection (IndexCollection.java:469) - Stemmer: porter\n",
            "2023-04-18 23:20:13,939 INFO  [main] index.IndexCollection (IndexCollection.java:470) - Keep stopwords? false\n",
            "2023-04-18 23:20:13,939 INFO  [main] index.IndexCollection (IndexCollection.java:471) - Stopwords file: null\n",
            "2023-04-18 23:20:14,037 INFO  [main] index.IndexCollection (IndexCollection.java:510) - Thread pool with 12 threads initialized.\n",
            "2023-04-18 23:20:14,037 INFO  [main] index.IndexCollection (IndexCollection.java:512) - Initializing collection in trec_covid_expanded_data\n",
            "2023-04-18 23:20:14,038 INFO  [main] index.IndexCollection (IndexCollection.java:521) - 1 file found\n",
            "2023-04-18 23:20:14,038 INFO  [main] index.IndexCollection (IndexCollection.java:522) - Starting to index...\n",
            "2023-04-18 23:20:14,070 ERROR [pool-2-thread-1] index.IndexCollection$LocalIndexerThread (IndexCollection.java:348) - pool-2-thread-1: Unexpected Exception:\n",
            "java.lang.RuntimeException: JSON document has no \"contents\" field!\n",
            "\tat io.anserini.collection.JsonCollection$Document.contents(JsonCollection.java:218) ~[anserini-0.21.0-fatjar.jar:?]\n",
            "\tat io.anserini.index.generator.DefaultLuceneDocumentGenerator.createDocument(DefaultLuceneDocumentGenerator.java:63) ~[anserini-0.21.0-fatjar.jar:?]\n",
            "\tat io.anserini.index.IndexCollection$LocalIndexerThread.run(IndexCollection.java:295) [anserini-0.21.0-fatjar.jar:?]\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]\n",
            "\tat java.lang.Thread.run(Thread.java:834) [?:?]\n",
            "2023-04-18 23:20:14,101 INFO  [main] index.IndexCollection (IndexCollection.java:578) - Indexing Complete! 0 documents indexed\n",
            "2023-04-18 23:20:14,101 INFO  [main] index.IndexCollection (IndexCollection.java:579) - ============ Final Counter Values ============\n",
            "2023-04-18 23:20:14,101 INFO  [main] index.IndexCollection (IndexCollection.java:580) - indexed:                0\n",
            "2023-04-18 23:20:14,101 INFO  [main] index.IndexCollection (IndexCollection.java:581) - unindexable:            0\n",
            "2023-04-18 23:20:14,101 INFO  [main] index.IndexCollection (IndexCollection.java:582) - empty:                  0\n",
            "2023-04-18 23:20:14,101 INFO  [main] index.IndexCollection (IndexCollection.java:583) - skipped:                0\n",
            "2023-04-18 23:20:14,102 INFO  [main] index.IndexCollection (IndexCollection.java:584) - errors:                 0\n",
            "2023-04-18 23:20:14,105 INFO  [main] index.IndexCollection (IndexCollection.java:587) - Total 0 documents indexed in 00:00:00\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Generating the indexes\n",
        "\n",
        "import pytrec_eval\n",
        "from pathlib import Path\n",
        "import os\n",
        "import subprocess\n",
        "import shutil\n",
        "\n",
        "\n",
        "trec_covid_expanded = []\n",
        "\n",
        "# Load the expanded queries\n",
        "with open(\"trec_covid_expanded.json\", \"r\") as f:\n",
        "    trec_covid_expanded = json.load(f)\n",
        "\n",
        "#Concatenate expanded queries to their respective documents\n",
        "for doc in trec_covid_expanded:\n",
        "    doc[\"text\"] = f\"{ doc['text']} {doc['expanded_query']}\"\n",
        "\n",
        "# Write the original and expanded TREC-COVID datasets to jsonl files\n",
        "with open(\"trec_covid_original.jsonl\", \"w\") as f:\n",
        "    for doc_id, text in zip(trec_covid_corpus[\"_id\"], trec_covid_corpus[\"text\"]):\n",
        "        doc = {\"id\": doc_id, \"text\": text}\n",
        "        f.write(json.dumps(doc) + \"\\n\")\n",
        "\n",
        "with open(\"trec_covid_expanded.jsonl\", \"w\") as f:\n",
        "    for doc in trec_covid_expanded:\n",
        "        f.write(json.dumps(doc) + \"\\n\")\n",
        "\n",
        "# Define the directories where the indexes will be stored\n",
        "expanded_index_directory = \"trec_covid_expanded_index\"\n",
        "original_index_directory = \"trec_covid_original_index\"\n",
        "\n",
        "# Create directories for original and expanded datasets\n",
        "original_data_directory = \"trec_covid_original_data\"\n",
        "expanded_data_directory = \"trec_covid_expanded_data\"\n",
        "\n",
        "# Create the directories if they do not exist\n",
        "Path(expanded_index_directory).mkdir(parents=True, exist_ok=True)\n",
        "Path(original_index_directory).mkdir(parents=True, exist_ok=True)\n",
        "Path(original_data_directory).mkdir(parents=True, exist_ok=True)\n",
        "Path(expanded_data_directory).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Move the jsonl files into their corresponding directories\n",
        "shutil.move(\"trec_covid_original.jsonl\", f\"{original_data_directory}/trec_covid_original.jsonl\")\n",
        "shutil.move(\"trec_covid_expanded.jsonl\", f\"{expanded_data_directory}/trec_covid_expanded.jsonl\")\n",
        "\n",
        "\n",
        "# Index the original dataset\n",
        "print('Index the original dataset')\n",
        "result = subprocess.run([\n",
        "    \"python\", \"-m\", \"pyserini.index.lucene\",\n",
        "    \"--collection\", \"JsonCollection\",\n",
        "    \"--input\", original_data_directory,\n",
        "    \"--index\", original_index_directory,\n",
        "    \"--generator\", \"DefaultLuceneDocumentGenerator\",\n",
        "    \"--threads\", \"12\",\n",
        "    \"--storePositions\", \"--storeDocvectors\", \"--storeRaw\"\n",
        "], capture_output=True, text=True)\n",
        "print(result.stdout)\n",
        "print(result.stderr)\n",
        "\n",
        "# Index the expanded dataset\n",
        "print('Index the expanded dataset')\n",
        "result = subprocess.run([\n",
        "    \"python\", \"-m\", \"pyserini.index.lucene\",\n",
        "    \"--collection\", \"JsonCollection\",\n",
        "    \"--input\", expanded_data_directory,\n",
        "    \"--index\", expanded_index_directory,\n",
        "    \"--generator\", \"DefaultLuceneDocumentGenerator\",\n",
        "    \"--threads\", \"12\",\n",
        "    \"--storePositions\", \"--storeDocvectors\", \"--storeRaw\"\n",
        "], capture_output=True, text=True)\n",
        "print(result.stdout)\n",
        "print(result.stderr)\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Load the indexes (either downloaded or built index)\n"
          ]
        },
        {
          "ename": "JavaException",
          "evalue": "No methods called search in io/anserini/search/SimpleSearcher matching your arguments, requested: (['what was the clinical features of 40 patients'], 10), available: ['(Ljava/lang/String;)[Lio/anserini/search/SimpleSearcher$Result;', '(Ljava/lang/String;I)[Lio/anserini/search/SimpleSearcher$Result;', '(Lio/anserini/search/query/QueryGenerator;Ljava/lang/String;I)[Lio/anserini/search/SimpleSearcher$Result;', '(Lorg/apache/lucene/search/Query;I)[Lio/anserini/search/SimpleSearcher$Result;']",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mJavaException\u001b[0m                             Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[4], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39m# Evaluate the BM25 performance on both indexes using nDCG@10\u001b[39;00m\n\u001b[0;32m     36\u001b[0m qrels \u001b[39m=\u001b[39m qrels_dict\n\u001b[1;32m---> 38\u001b[0m bm25_expanded_ndcg \u001b[39m=\u001b[39m evaluate_bm25_expansion(\n\u001b[0;32m     39\u001b[0m     expanded_searcher, query_list, trec_covid_expanded, qrels\n\u001b[0;32m     40\u001b[0m )\n\u001b[0;32m     41\u001b[0m bm25_no_expansion_ndcg \u001b[39m=\u001b[39m evaluate_bm25_no_expansion(\n\u001b[0;32m     42\u001b[0m     original_searcher, query_list, qrels\n\u001b[0;32m     43\u001b[0m )\n\u001b[0;32m     45\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnDCG@10 for BM25 without expansion: \u001b[39m\u001b[39m{\u001b[39;00mbm25_no_expansion_ndcg\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
            "Cell \u001b[1;32mIn[4], line 12\u001b[0m, in \u001b[0;36mevaluate_bm25_expansion\u001b[1;34m(searcher, queries, expanded_queries, qrels)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m query, expanded_query \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(queries, expanded_queries):\n\u001b[0;32m     11\u001b[0m     query_id \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(query[\u001b[39m\"\u001b[39m\u001b[39mquery_id\u001b[39m\u001b[39m\"\u001b[39m])  \u001b[39m# Updated\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     hits \u001b[39m=\u001b[39m searcher\u001b[39m.\u001b[39;49msearch(expanded_query[\u001b[39m\"\u001b[39;49m\u001b[39mexpanded_query\u001b[39;49m\u001b[39m\"\u001b[39;49m], k\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n\u001b[0;32m     13\u001b[0m     run[query_id] \u001b[39m=\u001b[39m {hit\u001b[39m.\u001b[39mdocid: idx \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m \u001b[39mfor\u001b[39;00m idx, hit \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(hits)}\n\u001b[0;32m     14\u001b[0m evaluator \u001b[39m=\u001b[39m pytrec_eval\u001b[39m.\u001b[39mRelevanceEvaluator(qrels, {\u001b[39m'\u001b[39m\u001b[39mndcg_cut\u001b[39m\u001b[39m'\u001b[39m})\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pyserini\\search\\lucene\\_searcher.py:143\u001b[0m, in \u001b[0;36mLuceneSearcher.search\u001b[1;34m(self, q, k, query_generator, fields, strip_segment_id, remove_dups)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    142\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fields:\n\u001b[1;32m--> 143\u001b[0m         hits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobject\u001b[39m.\u001b[39;49msearch(q, k)\n\u001b[0;32m    144\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    145\u001b[0m         hits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobject\u001b[39m.\u001b[39msearch_fields(q, jfields, k)\n",
            "File \u001b[1;32mjnius\\jnius_export_class.pxi:1163\u001b[0m, in \u001b[0;36mjnius.JavaMultipleMethod.__call__\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;31mJavaException\u001b[0m: No methods called search in io/anserini/search/SimpleSearcher matching your arguments, requested: (['what was the clinical features of 40 patients'], 10), available: ['(Ljava/lang/String;)[Lio/anserini/search/SimpleSearcher$Result;', '(Ljava/lang/String;I)[Lio/anserini/search/SimpleSearcher$Result;', '(Lio/anserini/search/query/QueryGenerator;Ljava/lang/String;I)[Lio/anserini/search/SimpleSearcher$Result;', '(Lorg/apache/lucene/search/Query;I)[Lio/anserini/search/SimpleSearcher$Result;']"
          ]
        }
      ],
      "source": [
        "# Evaluating\n",
        "import pyserini\n",
        "from pyserini.search import LuceneSearcher\n",
        "print('Load the indexes (either downloaded or built index)')\n",
        "expanded_searcher = LuceneSearcher(expanded_index_directory)\n",
        "original_searcher = LuceneSearcher(original_index_directory)\n",
        "\n",
        "def evaluate_bm25_expansion(searcher, queries, expanded_queries, qrels):\n",
        "    run = {}    \n",
        "    for query, expanded_query in zip(queries, expanded_queries):\n",
        "        query_id = str(query[\"query_id\"])  # Updated\n",
        "        hits = searcher.search(expanded_query[\"expanded_query\"], k=10)\n",
        "        run[query_id] = {hit.docid: idx + 1 for idx, hit in enumerate(hits)}\n",
        "    evaluator = pytrec_eval.RelevanceEvaluator(qrels, {'ndcg_cut'})\n",
        "    metrics = evaluator.evaluate(run)\n",
        "    ndcg = {key: value['ndcg_cut_10'] for key, value in metrics.items()}\n",
        "    mean_ndcg = sum(ndcg.values()) / len(ndcg)\n",
        "    return mean_ndcg\n",
        "\n",
        "def evaluate_bm25_no_expansion(searcher, queries, qrels):\n",
        "    run = {}\n",
        "    for query in queries:\n",
        "        query_id = str(query[\"query_id\"])  # Updated\n",
        "        hits = searcher.search(query[\"text\"], k=10)  # Updated\n",
        "        run[query_id] = {hit.docid: idx + 1 for idx, hit in enumerate(hits)}\n",
        "    evaluator = pytrec_eval.RelevanceEvaluator(qrels, {'ndcg_cut'})\n",
        "    metrics = evaluator.evaluate(run)\n",
        "    ndcg = {key: value['ndcg_cut_10'] for key, value in metrics.items()}\n",
        "    mean_ndcg = sum(ndcg.values()) / len(ndcg)\n",
        "    return mean_ndcg\n",
        "\n",
        "# Create a list of query dictionaries\n",
        "query_list = [{\"query_id\": query_id, \"text\": text} for query_id, text in zip(trec_covid_queries[\"_id\"], trec_covid_queries[\"text\"])]\n",
        "\n",
        "# Evaluate the BM25 performance on both indexes using nDCG@10\n",
        "qrels = qrels_dict\n",
        "\n",
        "bm25_expanded_ndcg = evaluate_bm25_expansion(\n",
        "    expanded_searcher, query_list, trec_covid_expanded, qrels\n",
        ")\n",
        "bm25_no_expansion_ndcg = evaluate_bm25_no_expansion(\n",
        "    original_searcher, query_list, qrels\n",
        ")\n",
        "\n",
        "print(f\"nDCG@10 for BM25 without expansion: {bm25_no_expansion_ndcg}\")\n",
        "print(f\"nDCG@10 for BM25 with expansion: {bm25_expanded_ndcg}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"trec_covid_queries (first 3 items):\")\n",
        "for key, value in list(trec_covid_queries.items())[:3]:\n",
        "    print(f\"{key}: {value}\")\n",
        "\n",
        "print(\"\\ntrec_covid_expanded (first item):\")\n",
        "print(trec_covid_expanded[0] if len(trec_covid_expanded) > 0 else \"Empty list\")\n",
        "\n",
        "print(\"\\nqrels_dict (first item):\")\n",
        "first_key = list(qrels_dict.keys())[0]\n",
        "print(f\"{first_key}: {qrels_dict[first_key]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trec_covid_qrels = load_dataset(\"BeIR/trec-covid\", \"queries\")\n",
        "a = trec_covid_qrels['queries']\n",
        "print(a['title'])\n",
        "print(a['text'])\n",
        "\n",
        "num_lines_to_print = 5\n",
        "\n",
        "for i in range(num_lines_to_print):\n",
        "    doc_id = trec_covid_corpus[\"id\"][i]\n",
        "    text = trec_covid_corpus[\"text\"][i]\n",
        "    print(f\"Document {i + 1}:\")\n",
        "    print(f\"ID: {doc_id}\")\n",
        "    print(f\"Text: {text}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trec_covid_qrels = load_dataset(\"BeIR/trec-covid\", \"qrels\")\n",
        "qrels_dict = {}\n",
        "for qrel in trec_covid_qrels[\"qrels\"]:\n",
        "    query_id = str(qrel[\"query_id\"])\n",
        "    doc_id = str(qrel[\"doc_id\"])\n",
        "    relevance = qrel[\"relevance\"]\n",
        "    if query_id not in qrels_dict:\n",
        "        qrels_dict[query_id] = {}\n",
        "    qrels_dict[query_id][doc_id] = relevance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print some relevant specs of the trec_covid_queries dictionary\n",
        "print(f\"Number of queries: {len(trec_covid_queries)}\")\n",
        "print(\"First 5 queries:\")\n",
        "for i in range(5):\n",
        "    query_id = trec_covid_queries[\"_id\"][i]\n",
        "    query_text = trec_covid_queries[\"text\"][i]\n",
        "    print(f\"Query {query_id}: {query_text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print the first three queries\n",
        "print(\"First three queries:\")\n",
        "for i in range(3):\n",
        "    print(f\"Query {trec_covid_queries['_id'][i]}: {trec_covid_queries['text'][i]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print some relevant specs of the trec_covid_queries dictionary\n",
        "print(f\"Number of queries: {len(trec_covid_queries)}\")\n",
        "print(\"First 5 queries:\")\n",
        "for i in range(5):\n",
        "    query_id = trec_covid_queries[\"_id\"][i]\n",
        "    query_text = trec_covid_queries[\"text\"][i]\n",
        "    print(f\"Query {query_id}: {query_text}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print the number of expanded queries and the text of the first 5 queries\n",
        "print(f\"Number of queries: {len(trec_covid_queries)}\")\n",
        "print(\"First 5 queries:\")\n",
        "for i in range(5):\n",
        "    print(f\"Query {trec_covid_queries['_id'][i]}: {trec_covid_queries['text'][i][:50]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print the number of queries and the relevant documents for the first 5 queries\n",
        "print(f\"Number of queries in qrels_dict: {len(qrels_dict)}\")\n",
        "print(\"Relevant documents for first 5 queries:\")\n",
        "for i in range(5):\n",
        "    query_id = list(qrels_dict.keys())[i]\n",
        "    relevant_docs = list(qrels_dict[query_id].keys())[:5]\n",
        "    print(f\"Query {query_id}: {relevant_docs}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
