{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRD6t-zi7Qec"
      },
      "outputs": [],
      "source": [
        "# Baixe o dataset do ms marco tiny\n",
        "#curl -O https://storage.googleapis.com/unicamp-dl/ia368dd_2023s1/msmarco/msmarco_triples.train.tiny.tsv\n",
        "\n",
        "\n",
        "#!pip3 install nmslib\n",
        "#!python -m pip install --upgrade pip\n",
        "#!pip install pyserini\n",
        "#!pip install datasets\n",
        "#!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117\n",
        "#!pip install sacrebleu \n",
        "#!pip install scikit-learn \n",
        "#!pip install torch\n",
        "#!pip install sentencepiece\n",
        "#!pip install transformers\n",
        "#!pip install pandas\n",
        "#!pip install torch\n",
        "#!pip install transformers\n",
        "#!pip install tqdm\n",
        "#!pip install sacrebleu\n",
        "#!pip install scikit-learn\n",
        "#!pip install jupyter notebok\n",
        "#!pip install jsonlines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wd1j56rx7R35"
      },
      "outputs": [],
      "source": [
        "!export PYTORCH_CUDA_ALLOC_CONF=garbage_collection_threshold:0.6\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config, AdamW\n",
        "from tqdm import tqdm\n",
        "import sacrebleu\n",
        "\n",
        "# Atualizar a classe MSMARCODataset\n",
        "class MSMARCODataset(Dataset):\n",
        "    def __init__(self, data_file, tokenizer, max_len):\n",
        "        self.data = pd.read_csv(data_file, delimiter=\"\\t\", header=None, names=[\"query\", \"relevant_passage\", \"non_relevant_passage\"])\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        query = self.data.loc[index, \"query\"]\n",
        "        relevant_passage = self.data.loc[index, \"relevant_passage\"]\n",
        "        tokenized_inputs = self.tokenizer(relevant_passage, return_tensors=\"pt\", max_length=self.max_len, padding=\"max_length\", truncation=True)\n",
        "        tokenized_outputs = self.tokenizer(query, return_tensors=\"pt\", max_length=self.max_len, padding=\"max_length\", truncation=True)\n",
        "        return {\"input_ids\": tokenized_inputs[\"input_ids\"].squeeze(0), \"attention_mask\": tokenized_inputs[\"attention_mask\"].squeeze(0), \"labels\": tokenized_outputs[\"input_ids\"].squeeze(0)}\n",
        "\n",
        "# Load the dataset and split it into training and validation sets\n",
        "data_file = \"msmarco_triples.train.tiny.tsv\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "max_len = 128\n",
        "dataset = MSMARCODataset(data_file, tokenizer, max_len)\n",
        "\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Create DataLoaders for training and validation datasets\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Initialize the model and optimizer\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Train the seq2seq model and validate every X steps  \n",
        "epochs = 2\n",
        "validate_every_x_steps = 50\n",
        "step_count = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_loss_accumulator = 0.0\n",
        "\n",
        "    for batch in tqdm(train_dataloader):\n",
        "        step_count += 1\n",
        "\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        train_loss_accumulator += loss.item()\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Validate the model every X steps\n",
        "        if step_count % validate_every_x_steps == 0:\n",
        "            model.eval()\n",
        "            val_loss_accumulator = 0.0\n",
        "            refs = []\n",
        "            hyps = []\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for val_batch in val_dataloader:\n",
        "                    val_input_ids = val_batch[\"input_ids\"].to(device)\n",
        "                    val_attention_mask = val_batch[\"attention_mask\"].to(device)\n",
        "                    val_labels = val_batch[\"labels\"].to(device)\n",
        "\n",
        "                    val_outputs = model(input_ids=val_input_ids, attention_mask=val_attention_mask, labels=val_labels)\n",
        "                    val_loss = val_outputs.loss\n",
        "                    val_loss_accumulator += val_loss.item()\n",
        "\n",
        "                    generated = model.generate(val_input_ids, attention_mask=val_attention_mask, max_length=max_len)\n",
        "                    hyps.extend(tokenizer.batch_decode(generated, skip_special_tokens=True))\n",
        "                    refs.extend(tokenizer.batch_decode(val_labels, skip_special_tokens=True))\n",
        "\n",
        "            val_loss_avg = val_loss_accumulator / len(val_dataloader)\n",
        "            train_loss_avg = train_loss_accumulator / validate_every_x_steps\n",
        "            bleu = sacrebleu.corpus_bleu(hyps, [refs])\n",
        "            print(f\"Step: {step_count}, Train Loss: {train_loss_avg}, Validation Loss: {val_loss_avg}, Validation BLEU: {bleu.score}\")\n",
        "\n",
        "            train_loss_accumulator = 0.0\n",
        "            model.train()\n",
        "\n",
        "model.save_pretrained(\"doc2query_model\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efX1elpM7eSX"
      },
      "source": [
        "## Gere as consultas expandidas para o TREC-COVID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip install datasets\n",
        "#!pip install pyserini\n",
        "\n",
        "a = trec_covid_corpus[\"corpus\"]\n",
        "b = trec_covid_queries[\"queries\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "import torch\n",
        "\n",
        "# Load the saved model\n",
        "model_path = \"doc2query_model\"\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\", model_max_length=128)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "trec_covid_corpus = load_dataset(\"BeIR/trec-covid\", \"corpus\")\n",
        "trec_covid_queries = load_dataset(\"BeIR/trec-covid\", \"queries\")\n",
        "trec_covid_expanded = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Available keys (splits):\", trec_covid_corpus.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print the first 5 entries\n",
        "for i in range(1000):\n",
        "    print(f\"Entry {i+1}:\")\n",
        "    print(trec_covid_corpus['corpus'][i])\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#sorted_corpus = sorted(trec_covid_corpus[\"corpus\"], key=lambda x: x[\"_id\"])[:500]\n",
        "# Print the first 5 entries\n",
        "for i in range(500):\n",
        "    print(f\"Entry {i+1}:\")\n",
        "    print(trec_covid_queries['queries'][i])\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "def generate_expanded_queries(document, model, tokenizer, max_len=128, batch_size=500):\n",
        "    # Tokenize the input\n",
        "    tokenized_inputs = tokenizer(document, return_tensors=\"pt\", max_length=max_len, padding=\"max_length\", truncation=True)\n",
        "    input_ids = tokenized_inputs[\"input_ids\"].to(device)\n",
        "    attention_mask = tokenized_inputs[\"attention_mask\"].to(device)\n",
        "\n",
        "    # Split the input into batches\n",
        "    input_ids_batches = torch.split(input_ids, batch_size)\n",
        "    attention_mask_batches = torch.split(attention_mask, batch_size)\n",
        "\n",
        "    # Initialize the output buffer\n",
        "    expanded_queries = []\n",
        "\n",
        "    for input_ids_batch, attention_mask_batch in zip(input_ids_batches, attention_mask_batches):\n",
        "        # Generate the output\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(input_ids=input_ids_batch, attention_mask=attention_mask_batch, max_new_tokens=max_len)\n",
        "        \n",
        "        # Decode the output\n",
        "        for output in outputs:\n",
        "            expanded_query = tokenizer.decode(output, skip_special_tokens=True)\n",
        "            expanded_queries.append(expanded_query)\n",
        "\n",
        "    return expanded_queries\n",
        "\n",
        "# Get 1000 sorted entries from the dataset\n",
        "#sorted_corpus = sorted(trec_covid_corpus[\"corpus\"], key=lambda x: x[\"_id\"])[:100]\n",
        "sorted_corpus = trec_covid_corpus[\"corpus\"]\n",
        "\n",
        "# Print the first 5 entries\n",
        "for i in range(5):\n",
        "    print(f\"Entry {i+1}:\")\n",
        "    print(sorted_corpus[i])\n",
        "    print()\n",
        "\n",
        "for doc in tqdm(sorted_corpus, desc=\"Generating queries\"):\n",
        "    expanded_query = generate_expanded_queries(doc[\"text\"], model, tokenizer, max_len=128, batch_size=500)\n",
        "    trec_covid_expanded.append({\"id\": doc[\"_id\"], \"text\": doc[\"text\"], \"expanded_query\": expanded_query})\n",
        "    #print(\"Original Document Text:\")\n",
        "    #print(doc[\"text\"])\n",
        "    #print(\"Expanded Queries:\")\n",
        "    #print(expanded_query)  \n",
        "    #print(f\"Generated {len(trec_covid_expanded)} queries\")\n",
        "\n",
        "# Salve as consultas expandidas em um arquivo\n",
        "with open(\"trec_covid_expanded.json\", \"w\") as f:\n",
        "    json.dump(trec_covid_expanded, f)    \n",
        "\n",
        "\n",
        "#for doc in tqdm(trec_covid_corpus[\"corpus\"], desc=\"Generating queries\"):\n",
        "#    expanded_query = generate_expanded_queries(doc[\"text\"], model, tokenizer, max_len=20, batch_size=10000)\n",
        "#    trec_covid_expanded.append({\"id\": doc[\"_id\"], \"text\": doc[\"text\"], \"expanded_query\": expanded_query})"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gerando o Indice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import jsonlines\n",
        "import os\n",
        "\n",
        "workdir = \"trec-covid/\"\n",
        "os.makedirs(workdir, exist_ok=True)\n",
        "\n",
        "json_batch_size = 1 #len(sorted_corpus) // 10\n",
        "j = 0\n",
        "\n",
        "for i in range(0, len(sorted_corpus), json_batch_size):\n",
        "    filename = f\"{workdir}json_{j}.json\"\n",
        "    print(filename)\n",
        "    with jsonlines.open(filename, mode='w') as writer:\n",
        "        for item in sorted_corpus[i:i + json_batch_size]:\n",
        "            writer.write(item)\n",
        "    j += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_BuTFQh7V8f"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pytrec_eval\n",
        "from pyserini.search import SimpleSearcher\n",
        "\n",
        "\n",
        "\n",
        "# Carregue o índice do TREC-COVID\n",
        "searcher = SimpleSearcher(\"beir-v1.0.0-trec-covid-flat\")\n",
        "\n",
        "# BM25 sem expansão\n",
        "def evaluate_bm25_no_expansion(searcher, trec_covid_queries, qrels, k=10):\n",
        "    evaluator = pytrec_eval.RelevanceEvaluator(qrels, {'map_cut', 'ndcg_cut', 'recip_rank'})\n",
        "    topics = {str(topic[\"id\"]): topic[\"query\"] for topic in trec_covid_queries[\"queries\"]}\n",
        "    qrun = {}\n",
        "    for topic_id, query in topics.items():\n",
        "        hits = searcher.search(query, k)\n",
        "        qrun[topic_id] = {hit.docid: idx + 1 for idx, hit in enumerate(hits)}\n",
        "    results = evaluator.evaluate(qrun)\n",
        "    return results['ndcg_cut_10']\n",
        "\n",
        "# BM25 com expansão\n",
        "def evaluate_bm25_expansion(searcher, trec_covid_queries, trec_covid_expanded, qrels, k=10):\n",
        "    evaluator = pytrec_eval.RelevanceEvaluator(qrels, {'map_cut', 'ndcg_cut', 'recip_rank'})\n",
        "    topics = {str(topic[\"id\"]): topic[\"query\"] for topic in trec_covid_queries[\"queries\"]}\n",
        "    expanded_queries = {doc[\"id\"]: doc[\"expanded_query\"] for doc in trec_covid_expanded}\n",
        "    qrun = {}\n",
        "    for topic_id, query in topics.items():\n",
        "        expanded_query = f\"{query} {expanded_queries[topic_id]}\"\n",
        "        hits = searcher.search(expanded_query, k)\n",
        "        qrun[topic_id] = {hit.docid: idx + 1 for idx, hit in enumerate(hits)}\n",
        "    results = evaluator.evaluate(qrun)\n",
        "    return results['ndcg_cut_10']\n",
        "\n",
        "qrels = {str(qrel[\"query_id\"]): {str(qrel[\"doc_id\"]): qrel[\"relevance\"] for qrel in trec_covid_queries[\"qrels\"]} for qrel in trec_covid_queries[\"queries\"]}\n",
        "\n",
        "bm25_no_expansion_ndcg = evaluate_bm25_no_expansion(searcher, trec_covid_queries, qrels)\n",
        "bm25_expansion_ndcg = evaluate_bm25_expansion(searcher, trec_covid_queries, trec_covid_expanded, qrels)\n",
        "\n",
        "print(f\"nDCG@10 do BM25 sem expansão: {bm25_no_expansion_ndcg}\")\n",
        "print(f\"nDCG@10 do BM25 com expansão: {bm25_expansion_ndcg}\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load the data for evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the data for evaluation\n",
        "\n",
        "from datasets import load_dataset\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "trec_covid_corpus = load_dataset(\"BeIR/trec-covid\", \"corpus\")\n",
        "trec_covid_queries = load_dataset(\"BeIR/trec-covid\", \"queries\")\n",
        "# Convert datasets to dictionaries\n",
        "trec_covid_corpus = trec_covid_corpus[\"corpus\"].to_dict()\n",
        "trec_covid_queries = trec_covid_queries[\"queries\"].to_dict()\n",
        "\n",
        "# Load the trec-covid-qrels dataset\n",
        "trec_covid_qrels = load_dataset(\"BeIR/trec-covid-qrels\")\n",
        "\n",
        "# Create a DataFrame to store the qrels data\n",
        "qrels = pd.DataFrame()\n",
        "qrels[\"query_id\"] = trec_covid_qrels['test'][\"query-id\"]\n",
        "qrels[\"corpus_id\"] = trec_covid_qrels['test'][\"corpus-id\"]\n",
        "qrels[\"score\"] = trec_covid_qrels['test'][\"score\"]\n",
        "\n",
        "# Create a dictionary from qrels data\n",
        "qrels_dict = {}\n",
        "for query_id, corpus_id, score in zip(qrels[\"query_id\"], qrels[\"corpus_id\"], qrels[\"score\"]):\n",
        "    query_id = str(query_id)\n",
        "    corpus_id = corpus_id.strip()\n",
        "    score = int(score)\n",
        "    if query_id not in qrels_dict:\n",
        "        qrels_dict[query_id] = {}\n",
        "    qrels_dict[query_id][corpus_id] = score\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Generating the indexes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generating the indexes\n",
        "\n",
        "import pytrec_eval\n",
        "from pathlib import Path\n",
        "import os\n",
        "import subprocess\n",
        "import shutil\n",
        "\n",
        "\n",
        "trec_covid_expanded = []\n",
        "\n",
        "# Load the expanded queries\n",
        "with open(\"trec_covid_expanded.json\", \"r\") as f:\n",
        "    trec_covid_expanded = json.load(f)\n",
        "\n",
        "#Concatenate expanded queries to their respective documents\n",
        "for doc in trec_covid_expanded:\n",
        "    doc[\"text\"] = f\"{ doc['text']} {doc['expanded_query']}\"\n",
        "\n",
        "# Write the original and expanded TREC-COVID datasets to jsonl files\n",
        "with open(\"trec_covid_original.jsonl\", \"w\") as f:\n",
        "    for doc_id, text in zip(trec_covid_corpus[\"_id\"], trec_covid_corpus[\"text\"]):\n",
        "        doc = {\"id\": doc_id, \"text\": text}\n",
        "        f.write(json.dumps(doc) + \"\\n\")\n",
        "\n",
        "with open(\"trec_covid_expanded.jsonl\", \"w\") as f:\n",
        "    for doc in trec_covid_expanded:\n",
        "        f.write(json.dumps(doc) + \"\\n\")\n",
        "\n",
        "# Define the directories where the indexes will be stored\n",
        "expanded_index_directory = \"trec_covid_expanded_index\"\n",
        "original_index_directory = \"trec_covid_original_index\"\n",
        "\n",
        "# Create directories for original and expanded datasets\n",
        "original_data_directory = \"trec_covid_original_data\"\n",
        "expanded_data_directory = \"trec_covid_expanded_data\"\n",
        "\n",
        "# Create the directories if they do not exist\n",
        "Path(expanded_index_directory).mkdir(parents=True, exist_ok=True)\n",
        "Path(original_index_directory).mkdir(parents=True, exist_ok=True)\n",
        "Path(original_data_directory).mkdir(parents=True, exist_ok=True)\n",
        "Path(expanded_data_directory).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Move the jsonl files into their corresponding directories\n",
        "shutil.move(\"trec_covid_original.jsonl\", f\"{original_data_directory}/trec_covid_original.jsonl\")\n",
        "shutil.move(\"trec_covid_expanded.jsonl\", f\"{expanded_data_directory}/trec_covid_expanded.jsonl\")\n",
        "\n",
        "\n",
        "# Index the original dataset\n",
        "print('Index the original dataset')\n",
        "result = subprocess.run([\n",
        "    \"python\", \"-m\", \"pyserini.index.lucene\",\n",
        "    \"--collection\", \"JsonCollection\",\n",
        "    \"--input\", original_data_directory,\n",
        "    \"--index\", original_index_directory,\n",
        "    \"--generator\", \"DefaultLuceneDocumentGenerator\",\n",
        "    \"--threads\", \"12\",\n",
        "    \"--storePositions\", \"--storeDocvectors\", \"--storeRaw\"\n",
        "], capture_output=True, text=True)\n",
        "print(result.stdout)\n",
        "print(result.stderr)\n",
        "\n",
        "# Index the expanded dataset\n",
        "print('Index the expanded dataset')\n",
        "result = subprocess.run([\n",
        "    \"python\", \"-m\", \"pyserini.index.lucene\",\n",
        "    \"--collection\", \"JsonCollection\",\n",
        "    \"--input\", expanded_data_directory,\n",
        "    \"--index\", expanded_index_directory,\n",
        "    \"--generator\", \"DefaultLuceneDocumentGenerator\",\n",
        "    \"--threads\", \"12\",\n",
        "    \"--storePositions\", \"--storeDocvectors\", \"--storeRaw\"\n",
        "], capture_output=True, text=True)\n",
        "print(result.stdout)\n",
        "print(result.stderr)\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluating\n",
        "import pyserini\n",
        "from pyserini.search import LuceneSearcher\n",
        "print('Load the indexes (either downloaded or built index)')\n",
        "expanded_searcher = LuceneSearcher(expanded_index_directory)\n",
        "original_searcher = LuceneSearcher(original_index_directory)\n",
        "\n",
        "def evaluate_bm25_expansion(searcher, queries, expanded_queries, qrels):\n",
        "    run = {}    \n",
        "    for query, expanded_query in zip(queries, expanded_queries):\n",
        "        query_id = str(query[\"query_id\"])  # Updated\n",
        "        hits = searcher.search(expanded_query[\"expanded_query\"], k=10)\n",
        "        run[query_id] = {hit.docid: idx + 1 for idx, hit in enumerate(hits)}\n",
        "    evaluator = pytrec_eval.RelevanceEvaluator(qrels, {'ndcg_cut'})\n",
        "    metrics = evaluator.evaluate(run)\n",
        "    ndcg = {key: value['ndcg_cut_10'] for key, value in metrics.items()}\n",
        "    mean_ndcg = sum(ndcg.values()) / len(ndcg)\n",
        "    return mean_ndcg\n",
        "\n",
        "def evaluate_bm25_no_expansion(searcher, queries, qrels):\n",
        "    run = {}\n",
        "    for query in queries:\n",
        "        query_id = str(query[\"query_id\"])  # Updated\n",
        "        hits = searcher.search(query[\"text\"], k=10)  # Updated\n",
        "        run[query_id] = {hit.docid: idx + 1 for idx, hit in enumerate(hits)}\n",
        "    evaluator = pytrec_eval.RelevanceEvaluator(qrels, {'ndcg_cut'})\n",
        "    metrics = evaluator.evaluate(run)\n",
        "    ndcg = {key: value['ndcg_cut_10'] for key, value in metrics.items()}\n",
        "    mean_ndcg = sum(ndcg.values()) / len(ndcg)\n",
        "    return mean_ndcg\n",
        "\n",
        "# Create a list of query dictionaries\n",
        "query_list = [{\"query_id\": query_id, \"text\": text} for query_id, text in zip(trec_covid_queries[\"_id\"], trec_covid_queries[\"text\"])]\n",
        "\n",
        "# Evaluate the BM25 performance on both indexes using nDCG@10\n",
        "qrels = qrels_dict\n",
        "\n",
        "bm25_expanded_ndcg = evaluate_bm25_expansion(\n",
        "    expanded_searcher, query_list, trec_covid_expanded, qrels\n",
        ")\n",
        "bm25_no_expansion_ndcg = evaluate_bm25_no_expansion(\n",
        "    original_searcher, query_list, qrels\n",
        ")\n",
        "\n",
        "print(f\"nDCG@10 for BM25 without expansion: {bm25_no_expansion_ndcg}\")\n",
        "print(f\"nDCG@10 for BM25 with expansion: {bm25_expanded_ndcg}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"trec_covid_queries (first 3 items):\")\n",
        "for key, value in list(trec_covid_queries.items())[:3]:\n",
        "    print(f\"{key}: {value}\")\n",
        "\n",
        "print(\"\\ntrec_covid_expanded (first item):\")\n",
        "print(trec_covid_expanded[0] if len(trec_covid_expanded) > 0 else \"Empty list\")\n",
        "\n",
        "print(\"\\nqrels_dict (first item):\")\n",
        "first_key = list(qrels_dict.keys())[0]\n",
        "print(f\"{first_key}: {qrels_dict[first_key]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trec_covid_qrels = load_dataset(\"BeIR/trec-covid\", \"queries\")\n",
        "a = trec_covid_qrels['queries']\n",
        "print(a['title'])\n",
        "print(a['text'])\n",
        "\n",
        "num_lines_to_print = 5\n",
        "\n",
        "for i in range(num_lines_to_print):\n",
        "    doc_id = trec_covid_corpus[\"id\"][i]\n",
        "    text = trec_covid_corpus[\"text\"][i]\n",
        "    print(f\"Document {i + 1}:\")\n",
        "    print(f\"ID: {doc_id}\")\n",
        "    print(f\"Text: {text}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trec_covid_qrels = load_dataset(\"BeIR/trec-covid\", \"qrels\")\n",
        "qrels_dict = {}\n",
        "for qrel in trec_covid_qrels[\"qrels\"]:\n",
        "    query_id = str(qrel[\"query_id\"])\n",
        "    doc_id = str(qrel[\"doc_id\"])\n",
        "    relevance = qrel[\"relevance\"]\n",
        "    if query_id not in qrels_dict:\n",
        "        qrels_dict[query_id] = {}\n",
        "    qrels_dict[query_id][doc_id] = relevance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print some relevant specs of the trec_covid_queries dictionary\n",
        "print(f\"Number of queries: {len(trec_covid_queries)}\")\n",
        "print(\"First 5 queries:\")\n",
        "for i in range(5):\n",
        "    query_id = trec_covid_queries[\"_id\"][i]\n",
        "    query_text = trec_covid_queries[\"text\"][i]\n",
        "    print(f\"Query {query_id}: {query_text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print the first three queries\n",
        "print(\"First three queries:\")\n",
        "for i in range(3):\n",
        "    print(f\"Query {trec_covid_queries['_id'][i]}: {trec_covid_queries['text'][i]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print some relevant specs of the trec_covid_queries dictionary\n",
        "print(f\"Number of queries: {len(trec_covid_queries)}\")\n",
        "print(\"First 5 queries:\")\n",
        "for i in range(5):\n",
        "    query_id = trec_covid_queries[\"_id\"][i]\n",
        "    query_text = trec_covid_queries[\"text\"][i]\n",
        "    print(f\"Query {query_id}: {query_text}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print the number of expanded queries and the text of the first 5 queries\n",
        "print(f\"Number of queries: {len(trec_covid_queries)}\")\n",
        "print(\"First 5 queries:\")\n",
        "for i in range(5):\n",
        "    print(f\"Query {trec_covid_queries['_id'][i]}: {trec_covid_queries['text'][i][:50]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print the number of queries and the relevant documents for the first 5 queries\n",
        "print(f\"Number of queries in qrels_dict: {len(qrels_dict)}\")\n",
        "print(\"Relevant documents for first 5 queries:\")\n",
        "for i in range(5):\n",
        "    query_id = list(qrels_dict.keys())[i]\n",
        "    relevant_docs = list(qrels_dict[query_id].keys())[:5]\n",
        "    print(f\"Query {query_id}: {relevant_docs}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
