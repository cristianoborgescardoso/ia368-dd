{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcN_5-RDWeqV"
      },
      "source": [
        "# Busca simples\n",
        "\n",
        "Desenvolvimento de um buscador Simples: Booleano, TF-IDF, BM25\n",
        "\n",
        "Tópicos abordados: Indexação, Bag-of-Words, TF-IDF, BM25\n",
        "\n",
        "Aula 1 - [Unicamp - IA368DD: Deep Learning aplicado a sistemas de busca.](https://www.cpg.feec.unicamp.br/cpg/lista/caderno_horario_show.php?id=1779)\n",
        "\n",
        "Autor: Marcus Vinícius Borela de Castro\n",
        "\n",
        "[Repositório no github](https://github.com/marcusborela/deep_learning_em_buscas_unicamp)\n",
        "\n",
        "[Link para chat de apoio com WebChatGPT](https://github.com/marcusborela/deep_learning_em_buscas_unicamp/blob/main/chat/CG%20uso%20no%20buscador%20aula%201.md)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ti1aFWTVgejM"
      },
      "source": [
        "[![Open In Colab latest github version](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/marcusborela/deep_learning_em_buscas_unicamp/blob/main/code/aula1_buscador_simples.ipynb) [Open In Colab latest github version]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQxzYKGgMqce"
      },
      "source": [
        "## Enunciado exercício\n",
        "\n",
        "Aula 2 - Notebook: Buscador Booleano/bag-of-words e buscador com TF-IDF\n",
        "\n",
        "1. Usar o BM25 implementado pelo pyserini para buscar queries no TREC-DL 2020\n",
        "Documentação referencia: https://github.com/castorini/pyserini/blob/master/docs/experiments-msmarco-passage.md\n",
        "2. Implementar um buscador booleano/bag-of-words.\n",
        "3. Implementar um buscador com TF-IDF\n",
        "4. Avaliar implementações 1, 2, e 3 no TREC-DL 2020 e calcular o nDCG@10\n",
        "Nos itens 2 e 3:\n",
        "\n",
        "Fazer uma implementação que suporta buscar eficientemente milhões de documentos.\n",
        "\n",
        "Não se pode usar bibliotecas como sklearn, que já implementam o BoW e TF-IDF.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmRLgbyi_Dvg"
      },
      "source": [
        "## Organizando o ambiente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ns_pq59lAHke",
        "outputId": "cdf9cc72-06ae-47cf-e825-5164fd90d991"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKAZ8CWCAM3-"
      },
      "outputs": [],
      "source": [
        "from psutil import virtual_memory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XgIWvkkH-kn"
      },
      "outputs": [],
      "source": [
        "def mostra_memoria():\n",
        "  vm = virtual_memory()\n",
        "  ram={}\n",
        "  ram['total']=round(vm.total / 1e9,2)\n",
        "  ram['available']=round(virtual_memory().available / 1e9,2)\n",
        "  # ram['percent']=round(virtual_memory().percent / 1e9,2)\n",
        "  ram['used']=round(virtual_memory().used / 1e9,2)\n",
        "  ram['free']=round(virtual_memory().free / 1e9,2)\n",
        "  ram['active']=round(virtual_memory().active / 1e9,2)\n",
        "  ram['inactive']=round(virtual_memory().inactive / 1e9,2)\n",
        "  ram['buffers']=round(virtual_memory().buffers / 1e9,2)\n",
        "  ram['cached']=round(virtual_memory().cached/1e9 ,2)\n",
        "  print(f\"Your runtime RAM in gb: \\n total {ram['total']}\\n available {ram['available']}\\n used {ram['used']}\\n free {ram['free']}\\n cached {ram['cached']}\\n buffers {ram['buffers']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dri9iiMAvCT",
        "outputId": "4db98aa6-46c0-4cb9-dac7-2833e6e584dc"
      },
      "outputs": [],
      "source": [
        "mostra_memoria()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9xRdgUGMPgh"
      },
      "source": [
        "### Vinculando pasta do google drive para salvar dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IsJiN6H8K6pe"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae-Iy2oz_9os",
        "outputId": "4849a217-f67c-42e9-f66c-152a3f8c60d2"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlGfGbtnBP0K",
        "outputId": "f830f6f8-79bf-44cb-c93b-8952e5af72f1"
      },
      "outputs": [],
      "source": [
        "!ls '/content/drive'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GYGL4MV_yhQ",
        "outputId": "7b305452-fb5b-4d2b-fe59-7daf07e203bc"
      },
      "outputs": [],
      "source": [
        "current_dir = os.getcwd()\n",
        "print(\"Current directory:\", current_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5MRuHo8md3Z"
      },
      "source": [
        "### Instalações de libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2af9dTbmPff",
        "outputId": "72bb72aa-f2fe-4380-f0ee-6ea021f5dd01"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/castorini/pygaggle.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTTVf-nYmc0w",
        "outputId": "7db3e6ac-fc6c-4113-dabe-ea1f5eea7bc7"
      },
      "outputs": [],
      "source": [
        "!pip install pyserini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3U_q5n1n_BW",
        "outputId": "f7c0b34d-e8a1-413b-f6ff-3711e52ec145"
      },
      "outputs": [],
      "source": [
        "!pip install faiss-cpu -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMp1ME6UK8vi"
      },
      "source": [
        "### Baixando o repositório do pyserini para usara seus scripts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGyBaf54LwCp"
      },
      "outputs": [],
      "source": [
        "path_pyserini = '/content/drive/MyDrive/treinamento/202301_IA368DD/code/pyserini'\n",
        "path_pyserini_tools = path_pyserini + '/pyserini-master/anserini-tools-master'\n",
        "path_pyserini_eval = path_pyserini + '/pyserini-master/pyserini/eval'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVBzuIuVMCqK"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(path_pyserini):\n",
        "    os.makedirs(path_pyserini)\n",
        "    print('pasta criada')\n",
        "    !wget -q https://github.com/castorini/pyserini/archive/refs/heads/master.zip -O pyserini.zip \n",
        "    !unzip -q pyserini.zip -d  {path_pyserini}\n",
        "    # Baixando tools que é um atalho para https://github.com/castorini/anserini-tools\n",
        "    !wget -q https://github.com/castorini/anserini-tools/archive/refs/heads/master.zip -O anserini-tools.zip \n",
        "    !unzip -q anserini-tools.zip -d  {path_pyserini}\n",
        "path_pyserini = path_pyserini + '/pyserini-master'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLHh0Nh8PW2E"
      },
      "outputs": [],
      "source": [
        " assert os.path.exists(path_pyserini), f\"Pasta {path_pyserini} não criada!\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30pi-eYwQ4zo"
      },
      "outputs": [],
      "source": [
        " assert os.path.exists(path_pyserini_tools), f\"Pasta {path_pyserini_tools} não criada!\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kF1v31OuUzpM"
      },
      "outputs": [],
      "source": [
        " assert os.path.exists(path_pyserini_eval), f\"Pasta {path_pyserini_eval} não criada!\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBfxPDhaFq5W"
      },
      "source": [
        "## Carga dos dados da TREC 2020 usando pyserini"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4gKjhiICmAL"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGPQPU38MbZc"
      },
      "source": [
        "### Obtendo dados dos documentos a partir do pyserini\n",
        "\n",
        "\n",
        "[Dicas aqui](https://github.com/castorini/pyserini/blob/master/docs/experiments-msmarco-passage.md)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WGHMsGcB7Ep"
      },
      "outputs": [],
      "source": [
        "path_data = '/content/drive/MyDrive/treinamento/202301_IA368DD/collections/msmarco-passage'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDb6qDbsA38j"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "if not os.path.exists(path_data):\n",
        "  os.makedirs(path_data)\n",
        "  print('pasta criada')\n",
        "  !wget https://msmarco.blob.core.windows.net/msmarcoranking/collectionandqueries.tar.gz -P {path_data}\n",
        "  !tar xvfz {path_data}/collectionandqueries.tar.gz -C {path_data}\n",
        "  os.remove(f'{path_data}/collectionandqueries.tar.gz')\n",
        "  print(\"Dados carregados!\")\n",
        "else:\n",
        "  print(\"Dados já existiam!\")    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5g4HhyAPx-4"
      },
      "outputs": [],
      "source": [
        " assert os.path.exists(path_data), f\"Pasta {path_data} não criada!\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzPgONt7ERjz"
      },
      "source": [
        "Passo anterior gera os seguintes arquivos:\n",
        "\n",
        "* collection.tsv\n",
        "* qrels.dev.small.tsv\n",
        "* qrels.train.tsv\n",
        "* queries.dev.small.tsv\n",
        "* queries.dev.tsv\n",
        "* queries.eval.small.tsv\n",
        "* queries.eval.tsv\n",
        "* queries.train.tsv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiNtb0oVDTPp"
      },
      "source": [
        "Next, we need to convert the MS MARCO tsv collection into Pyserini's jsonl files (which have one json object per line):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Cf9nEQS_iWs",
        "outputId": "601c3ea1-4e0a-4a06-bf0c-5324c509d3d2"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "if not os.path.exists(f'{path_data}/collection_jsonl'):\n",
        "  !python {path_pyserini_tools}/tools/scripts/msmarco/convert_collection_to_jsonl.py \\\n",
        "  --collection-path {path_data}/collection.tsv \\\n",
        "  --output-folder {path_data}/collection_jsonl\n",
        "  print(\"Dados carregados!\")\n",
        "else:\n",
        "  print(\"Dados já existiam!\")    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWbCv1UAP3yc"
      },
      "outputs": [],
      "source": [
        "assert os.path.exists(f'{path_data}/collection_jsonl'), f\"Pasta {path_data}/collection_jsonl não criada!\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4snYshcDmXI"
      },
      "source": [
        "The above script should generate 9 jsonl files in collections/msmarco-passage/collection_jsonl, each with 1M lines (except for the last one, which should have 841,823 lines)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wY35wjJNt99"
      },
      "source": [
        "Convertendo as queries (small dev) para o formato trec para avaliações futuras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXahbDquMWvV"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(f'{path_data}/qrels.dev.small.trec'):\n",
        "  !python {path_pyserini_tools}/scripts/msmarco/convert_msmarco_to_trec_qrels.py \\\n",
        "  --input {path_pyserini_tools}/topics-and-qrels/qrels.msmarco-passage.dev-subset.txt \\\n",
        "  --output {path_data}/qrels.dev.small.trec\n",
        "  print(\"Conversão efetuada!\")\n",
        "else:\n",
        "  print(\"Arquivo já existia!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTn273M0FTTH"
      },
      "source": [
        "### Loading data in dicts\n",
        "\n",
        "The 6980 queries in the development set are already stored in the repo. Let's take a peek:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-WOE8VZPfzf",
        "outputId": "8905dbb6-4a87-49cf-890c-a22ed360af51"
      },
      "outputs": [],
      "source": [
        "!head {path_pyserini_tools}/topics-and-qrels/topics.msmarco-passage.dev-subset.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "418Ih9pyiABT",
        "outputId": "bce16c08-4e2e-41e0-b119-f574841309ee"
      },
      "outputs": [],
      "source": [
        "!head {path_pyserini_tools}/topics-and-qrels/topics.dl20.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxlsspHzRu0o",
        "outputId": "d5b87b53-0be5-41b8-a63a-f1e939b66682"
      },
      "outputs": [],
      "source": [
        "!head {path_data}/qrels.dev.small.trec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-mW3XUeauti",
        "outputId": "8d8fb576-5506-4559-fd5a-26e271fa8c16"
      },
      "outputs": [],
      "source": [
        "!head {path_pyserini_tools}/topics-and-qrels/qrels.msmarco-passage.dev-subset.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoeVZrqXbGZo",
        "outputId": "3753d593-eecb-4d84-ae42-4a6262c6cd17"
      },
      "outputs": [],
      "source": [
        "!head {path_pyserini_tools}/topics-and-qrels/qrels.dl20-passage.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9XSzW8uHtic"
      },
      "source": [
        "Each line contains a tab-delimited (query id, query) pair. Conveniently, Pyserini already knows how to load and iterate through these pairs. We can now perform retrieval using these queries:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qiloI8loxJ2"
      },
      "source": [
        "#### Carregando queries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haoN-F06wryO"
      },
      "source": [
        "##### Carregando do arquivo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6ktuslxQ7QL"
      },
      "outputs": [],
      "source": [
        "def ler_arquivo_query_trec20(file_path:str):\n",
        "  \"\"\"\n",
        "  Função para ler um arquivo de queries TREC 2020 e retorná-las em um dicionário.\n",
        "\n",
        "  Args:\n",
        "    file_path (str): Caminho do arquivo de queries TREC 2020\n",
        "\n",
        "  Returns:\n",
        "    dict: Dicionário em que as chaves são os IDs das queries e os valores são os\n",
        "          textos das queries correspondentes.\n",
        "  \"\"\"\n",
        "\n",
        "  # Cria um dicionário vazio para armazenar as queries\n",
        "  query_dict = {}\n",
        "\n",
        "  # Abre o arquivo em modo leitura\n",
        "  with open(file_path, 'r') as f:\n",
        "      \n",
        "      # Itera sobre as linhas do arquivo\n",
        "      for line in f:\n",
        "\n",
        "          # Separa a linha em duas partes (id e texto), considerando que são separadas por uma tabulação\n",
        "          query_id, query_text = line.strip().split('\\t')\n",
        "          query_id = int(query_id)\n",
        "          # Adiciona a query ao dicionário, usando o id como chave e o texto como valor\n",
        "          query_dict[query_id] = query_text\n",
        "\n",
        "  # Retorna o dicionário com as queries\n",
        "  return query_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDnaMYIji-NL"
      },
      "source": [
        "Verificando queries de todo o dev dataset (total 6980)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAxjWWwoRR8a"
      },
      "outputs": [],
      "source": [
        "query_dev_dict = ler_arquivo_query_trec20(f'{path_pyserini_tools}/topics-and-qrels/topics.msmarco-passage.dev-subset.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fM9c3tbeidhU",
        "outputId": "041784f6-0b42-4917-b7ca-648871d76343"
      },
      "outputs": [],
      "source": [
        "len(query_dev_dict),list(query_dev_dict.items())[:4]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4ns_AJ7jDGC"
      },
      "source": [
        "Carregando o queries do trec20 dataset (total 200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrNCbYWpiHE5"
      },
      "outputs": [],
      "source": [
        "query_trec20_dict = ler_arquivo_query_trec20(f'{path_pyserini_tools}/topics-and-qrels/topics.dl20.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7kG-2z0iMQ7",
        "outputId": "e7226e52-9e30-4891-b3ba-44af96c93087"
      },
      "outputs": [],
      "source": [
        "len(query_trec20_dict),list(query_trec20_dict.items())[:4]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emUYIElNoHmP"
      },
      "source": [
        "##### Carregando usando get_topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ipt1M8UBoSiP"
      },
      "outputs": [],
      "source": [
        "from pyserini.search import get_topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ei_BuJtsoTjm",
        "outputId": "52a6052f-6ab5-4e7d-b21c-746752b6bf37"
      },
      "outputs": [],
      "source": [
        "topics = get_topics('dl20')\n",
        "print(f'{len(topics)} queries total')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4m7xTZe-oVNF",
        "outputId": "682ef9e9-79f1-4694-d862-9b0b585f56ac"
      },
      "outputs": [],
      "source": [
        "len(topics), list(topics.items())[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pw6AV1W3osR5"
      },
      "source": [
        "#### Carregando qrel (relevância por query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQWG3iK-wbHn"
      },
      "source": [
        "##### Carregando do arquivo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOmEAdTNRkbB"
      },
      "outputs": [],
      "source": [
        "def ler_arquivo_qrels_trec20(file_path:str) -> dict:\n",
        "    \"\"\"\n",
        "    Lê um arquivo TSV contendo a avaliação de relevância de documentos para cada consulta.\n",
        "    \n",
        "    Args:\n",
        "    file_path: str - O caminho do arquivo a ser lido.\n",
        "    \n",
        "    Returns:\n",
        "    dict - Um dicionário onde as chaves são os IDs das consultas e os valores são \n",
        "           dicionários em que as chaves são os IDs dos documentos e os valores são \n",
        "           os níveis de relevância (0, 1, 2, 3, ou 4) de cada documento para a consulta correspondente.\n",
        "    \"\"\"\n",
        "    qrels_dict = {}\n",
        "\n",
        "    with open(file_path, 'r') as f:\n",
        "        # Itera sobre cada linha do arquivo\n",
        "        for line in f:\n",
        "            # Separa a linha em seus campos\n",
        "            query_id, _, doc_id, relevance = line.strip().split()\n",
        "            query_id = int(query_id)\n",
        "            doc_id = int(doc_id)\n",
        "            # Verifica se a consulta já existe no dicionário\n",
        "            if query_id not in qrels_dict:\n",
        "                qrels_dict[query_id] = {}\n",
        "            # Adiciona o ID do documento e seu nível de relevância\n",
        "            qrels_dict[query_id][doc_id] = int(relevance)\n",
        "    return qrels_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9g2vYX4iwZ0"
      },
      "source": [
        "Verificando qrel de todo o dev dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWh0C-NkRkW8"
      },
      "outputs": [],
      "source": [
        "qrel_dev_dict = ler_arquivo_qrels_trec20(f'{path_data}/qrels.dev.small.trec')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlwCxvUEdVUo",
        "outputId": "d8a5666a-e481-4cfb-f9d1-14539a6f271e"
      },
      "outputs": [],
      "source": [
        "len(qrel_dev_dict),list(qrel_dev_dict.items())[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvaowThZi0oi"
      },
      "source": [
        "Carregando o qrel do trec20 dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCcNt3BVbbFf"
      },
      "outputs": [],
      "source": [
        "qrel_dev_dict = ler_arquivo_qrels_trec20(f'{path_pyserini_tools}/topics-and-qrels/qrels.dl20-passage.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WReZ3_qioXl",
        "outputId": "59da5b07-265d-4842-a26b-5bd512e1abc5"
      },
      "outputs": [],
      "source": [
        "len(qrel_dev_dict),list(qrel_dev_dict.items())[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CYRf021d0-n"
      },
      "source": [
        "Todas as 54 queries possuem informação de relevância"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jO2VRwxAUWZb",
        "outputId": "013a1d1c-f1f1-434b-b00d-fe8a545b9495"
      },
      "outputs": [],
      "source": [
        "[query for query, doc_rel in list(qrel_dev_dict.items()) if len(doc_rel)==0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6zEVa4Hv2bv"
      },
      "source": [
        "##### Carregando usando get_qrels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSq2Y3xavsFr"
      },
      "outputs": [],
      "source": [
        "from pyserini.search import get_qrels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CR6PwgD2vv0Y"
      },
      "outputs": [],
      "source": [
        "qrels = get_qrels('dl20-passage')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htSknV23wOuX",
        "outputId": "e462b39d-ebf6-43d8-e269-70f5c7aa49a0"
      },
      "outputs": [],
      "source": [
        "len(qrels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtkEsaTwwSJ-",
        "outputId": "8cc2eb44-33bd-42a6-cc85-a0817d6e9a90"
      },
      "outputs": [],
      "source": [
        "list(qrels.items())[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fCVAuEgMaYE"
      },
      "source": [
        "### Indexando Trec 2020 Collection using Pyserini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvftkvDAFdKr",
        "outputId": "4ce547b8-1a65-4ecb-da4a-6fcb2a904ec2"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "if not os.path.exists('./indexes/lucene-index-msmarco-passage'):\n",
        "  !python -m pyserini.index.lucene \\\n",
        "  --collection JsonCollection \\\n",
        "  --input {path_data}/collection_jsonl \\\n",
        "  --index indexes/lucene-index-msmarco-passage \\\n",
        "  --generator DefaultLuceneDocumentGenerator \\\n",
        "  --threads 9 \\\n",
        "  --storePositions --storeDocvectors --storeRaw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9LG2KrpGpaO",
        "outputId": "b0447e48-c5cc-4a66-f4e1-405388b630db"
      },
      "outputs": [],
      "source": [
        "!du -hs './indexes/lucene-index-msmarco-passage'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-EQ6_F2VktR"
      },
      "source": [
        "## Calculando ndcg@10 pelo pyserini no trec 2020 (small dev)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8y_HkjXlPTuL"
      },
      "source": [
        "#### Com script"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYrRnzZ7Sn3N"
      },
      "source": [
        "We can also use the official TREC evaluation tool, trec_eval, to compute metrics other than MRR@10. For that we first need to convert the run file into TREC format:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5WQoUwId-QY",
        "outputId": "617f0425-f86f-4f15-fe65-bd7256797a4d"
      },
      "outputs": [],
      "source": [
        "# trocar abaixo se for para realizar o search todo o dev dataset\n",
        "# file_topics = 'msmarco-passage-dev-subset'\n",
        "file_topics_search = 'dl20'\n",
        "print(f'file_topics_search: {file_topics_search}')\n",
        "\n",
        "# trocar abaixo se for para realizar o eval todo o dev dataset\n",
        "#file_topics_eval = {path_data}/qrels.dev.small.trec\n",
        "file_topics_eval = 'dl20-passage'\n",
        "print(f'file_topics_eval: {file_topics_eval}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbJjFHO4HY_4"
      },
      "outputs": [],
      "source": [
        "num_max_hits = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWYHSXe3Hw9o",
        "outputId": "2c8bb952-4a74-4e06-f77e-e9ebf14849fa"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "!python -m pyserini.search.lucene \\\n",
        "  --index indexes/lucene-index-msmarco-passage \\\n",
        "  --topics {file_topics_search} \\\n",
        "  --output runs/run.msmarco-passage.bm25.trec \\\n",
        "  --output-format msmarco \\\n",
        "  --hits {num_max_hits} \\\n",
        "  --bm25 --k1 0.82 --b 0.68"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Hegn5wfmjp4"
      },
      "source": [
        "Here, we set the BM25 parameters to k1=0.82, b=0.68 (tuned by grid search). The option --output-format msmarco says to generate output in the MS MARCO output format. The option --hits specifies the number of documents to return per query. Thus, the output file should have approximately 6980 × num_max_hits (698.000, if it is 100) lines.\n",
        "\n",
        "Retrieval speed will vary by hardware: On a reasonably modern CPU with an SSD, we might get around 13 qps (queries per second), and so the entire run should finish in under ten minutes (using a single thread). We can perform multi-threaded retrieval by using the --threads and --batch-size arguments. For example, setting --threads 16 --batch-size 64 on a CPU with sufficient cores, the entire run will finish in a couple of minutes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1GVBSGarYMH"
      },
      "source": [
        "Usamos parâmetro -l 2 seguindo orientação em pyserini\\docs\\experiments-msmarco-irst.md\n",
        "\n",
        "(...)\n",
        "Similarly, for TREC DL 2020:\n",
        "\n",
        "```bash\n",
        "python -m pyserini.eval.trec_eval -c -m map -m ndcg_cut.10 -l 2 \\\n",
        "  dl20-passage runs/run.irst-sum.passage.dl20.txt\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W55iQ9twkfUa",
        "outputId": "b807b0f0-c821-4e35-ebe5-0e07b18c93d7"
      },
      "outputs": [],
      "source": [
        "!python -m pyserini.eval.trec_eval -c -m ndcg_cut.10 -mrecall.100 -mmap -l 2 {file_topics_eval} runs/run.dl20-passage.bm25.trec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EiCRWGDZftVH",
        "outputId": "1c71f4e2-3197-48dc-a6b5-83e97c002581"
      },
      "outputs": [],
      "source": [
        "!python -m pyserini.search.lucene \\\n",
        "  --index indexes/lucene-index-msmarco-passage \\\n",
        "  --topics {file_topics} \\\n",
        "  --output runs/run.dl20-passage.bm25.trec \\\n",
        "  --output-format msmarco \\\n",
        "  --hits {num_max_hits} \\\n",
        "  --bm25 --k1 0.82 --b 0.68"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfXHH9upIVrL"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTQQt-qSINsY",
        "outputId": "e3bfb480-b6b0-4696-a430-c7045d2f98d2"
      },
      "outputs": [],
      "source": [
        "mostra_memoria()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waqMpGVHPVoi"
      },
      "source": [
        "#### Com código\n",
        "\n",
        "Adaptado do [caderno do colega Gustavo Bartz Guedes](https://colab.research.google.com/drive/10z86PObSxqbXczZ9pz0T-QurL21oMShf?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1z9TOmGucGBi"
      },
      "outputs": [],
      "source": [
        "# code from https://colab.research.google.com/github/castorini/anserini-notebooks/blob/master/pyserini_msmarco_passage_demo.ipynb\n",
        "from pyserini.search import SimpleSearcher\n",
        "from pyserini.search.lucene import LuceneSearcher\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUJuMaOKPmoZ"
      },
      "outputs": [],
      "source": [
        "# Run all queries in topics, retrive top 1k for each query\n",
        "def run_all_queries(file, topics, searcher, num_max_hits=100):\n",
        "    with open(file, 'w') as runfile:\n",
        "        cnt = 0\n",
        "        print('Running {} queries in total'.format(len(topics)))\n",
        "        for id in tqdm(topics, desc='Running Queries'):\n",
        "            query = topics[id]['title']\n",
        "            hits = searcher.search(query, num_max_hits)\n",
        "            for i in range(0, len(hits)):\n",
        "                _ = runfile.write('{} Q0 {} {} {:.6f} Pyserini\\n'.format(id, hits[i].docid, i+1, hits[i].score))\n",
        "            cnt += 1\n",
        "            if cnt % 100 == 0:\n",
        "                print(f'{cnt} queries completed')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkpqRcVwneUh"
      },
      "outputs": [],
      "source": [
        "searcher = LuceneSearcher('./indexes/lucene-index-msmarco-passage')\n",
        "searcher.set_bm25(k1=0.82, b=0.68)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yBalAPOnfv_",
        "outputId": "429ce3a2-06e3-47ef-8af6-56c97aaea033"
      },
      "outputs": [],
      "source": [
        "run_all_queries('run-msmarco-passage-bm25.txt', topics, searcher, num_max_hits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8Qp6adrpQh1",
        "outputId": "31bacfe3-9227-4e04-ed59-b70830d7659f"
      },
      "outputs": [],
      "source": [
        "!head run-msmarco-passage-bm25.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJAEtyT_e5wv"
      },
      "source": [
        "##### Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uR7eFP_6ehXY",
        "outputId": "12a59e8d-d320-4288-e37e-8e26da21ee83"
      },
      "outputs": [],
      "source": [
        "!python -m pyserini.eval.trec_eval -c -m ndcg_cut.10 -mrecall.100 -mmap -l 2 {file_topics_eval} run-msmarco-passage-bm25.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qs-31jR1pyas"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LStnoEzYt5gK"
      },
      "source": [
        "## Pré-processar as queries e os documentos\n",
        "\n",
        "[Seguindo padrão do lucene](docs/usage-analyzer.md) (Analyzer API)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHvD_P1Vt6N4"
      },
      "outputs": [],
      "source": [
        "from pyserini.analysis import Analyzer, get_lucene_analyzer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jbXDW8Qt6nH",
        "outputId": "b4a5fee6-9462-4e01-f7c0-8c56540db1d9"
      },
      "outputs": [],
      "source": [
        "# Default analyzer for English uses the Porter stemmer:\n",
        "analyzer = Analyzer(get_lucene_analyzer())\n",
        "tokens = analyzer.analyze('City buses are running on time.')\n",
        "print(tokens)\n",
        "# Result is ['citi', 'buse', 'run', 'time']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKe7NxQsukFr"
      },
      "outputs": [],
      "source": [
        "assert len(qrels)==54, \"qrels não carregado com relevância de 54 queries\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAGJ9Ju0ukCu"
      },
      "outputs": [],
      "source": [
        "assert len(topics)==200, \"topics não carregado com 200 queries\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XiGui9230fJ",
        "outputId": "323fb0a1-1c37-41f5-d66c-d69adc57b2b8"
      },
      "outputs": [],
      "source": [
        "list(topics.items())[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5qMvV0aD3lY"
      },
      "source": [
        "Para economizar esforço, retiraremos de topic as queries que não possuem informação de relevância, que não estão em qrel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Px3AxQY2ECi2"
      },
      "outputs": [],
      "source": [
        "topics_com_relevancia = {key:value for key, value in topics.items() if key in qrels}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ViCKtdnEgfJ",
        "outputId": "fdb186f2-edc7-4887-f3db-abbd67ae74d5"
      },
      "outputs": [],
      "source": [
        "len(topics_com_relevancia)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4YrjjSPEGGF"
      },
      "source": [
        "### Preprocessar documentos da coleção"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmElpsV1uj_o",
        "outputId": "fef08563-3932-4479-81b1-947721233db4"
      },
      "outputs": [],
      "source": [
        "!head {path_data}/collection_jsonl/docs08.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TmKi518euj8Q"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import List"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxtUkfWR0OTS"
      },
      "outputs": [],
      "source": [
        "# Define a função para pré-processar os documentos\n",
        "def preprocessar(text: str) -> List[str]:\n",
        "    # Aqui entra o código para pré-processar o texto\n",
        "    return analyzer.analyze(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5fm52md0ZS-"
      },
      "outputs": [],
      "source": [
        "# Define o caminho para a pasta com os arquivos JSON\n",
        "path_json_passage = f'{path_data}/collection_jsonl'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ottyzAxd_muZ"
      },
      "outputs": [],
      "source": [
        "def preprocessa_documentos(path_json_passage: str):\n",
        "  \"\"\"\n",
        "  Lê arquivos json no diretório `path_json_passage`, pré-processa os conteúdos (chave 'contents')\n",
        "  utilizando a função `preprocessar` e salva os novos arquivos com o mesmo nome, mas com sufixo \"_prep\".\n",
        "\n",
        "  Args:\n",
        "    path_json_passage (str): Caminho para o diretório contendo arquivos json.\n",
        "\n",
        "  \"\"\"\n",
        "  # Iterar sobre todos os arquivos no diretório\n",
        "  for file_name in tqdm(os.listdir(path_json_passage), desc=f'iterando arquivos json em  {path_json_passage}'):\n",
        "    if file_name.endswith('_prep.json'):\n",
        "        continue\n",
        "\n",
        "    print(f'Processando arquivo {file_name}')\n",
        "    \n",
        "    # Abrir o arquivo atual para leitura\n",
        "    file_path = os.path.join(path_json_passage, file_name)\n",
        "    print('em preprocess_document_lines', file_path)\n",
        "    with open(file_path, 'r') as f:\n",
        "        docs_json = {}\n",
        "        # Ler cada linha do arquivo (que contém um json)\n",
        "        for line in tqdm(f, desc=f'acessando {file_path}', total=1000000, miniters=100000):\n",
        "          doc = json.loads(line)\n",
        "          # Adicionar id do documento e seus tokens pré-processados no dicionário\n",
        "          docs_json[int(doc['id'])] = preprocessar(doc['contents'])\n",
        "\n",
        "    # Salvar arquivo pré-processado com novo nome\n",
        "    new_file_name = os.path.splitext(file_name)[0] + '_prep.json'\n",
        "    print(f'Gravando arquivo {new_file_name}')\n",
        "    with open(os.path.join(path_json_passage, new_file_name), 'w') as f:\n",
        "        json.dump(docs_json, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1i-GLteH_oHS",
        "outputId": "f16cf55a-e7e8-47d5-c129-39b68d0daab1"
      },
      "outputs": [],
      "source": [
        "preprocessa_documentos(path_json_passage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqsGYh5469-P",
        "outputId": "75254aae-cdcc-4d11-a322-100acf771c90"
      },
      "outputs": [],
      "source": [
        "mostra_memoria()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2CsmBfPGrRl"
      },
      "outputs": [],
      "source": [
        "def concatena_jsons(path):\n",
        "    dict_trec2020 = {}\n",
        "    for file_name in os.listdir(path):\n",
        "        if file_name.endswith('_prep.json'):\n",
        "            print(f'processando {file_name} ')\n",
        "            with open(os.path.join(path, file_name), 'r') as f:\n",
        "                dict_trec2020.update(json.load(f))\n",
        "    return dict_trec2020"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FElFQvvSFQIW",
        "outputId": "f48ab84b-1e68-4aff-9685-706f318011c6"
      },
      "outputs": [],
      "source": [
        "doctos_trec2020_dict = concatena_jsons(path_json_passage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ie6nK_9qIJAd",
        "outputId": "22518a8f-6616-4636-ed46-ca5414fd7ef4"
      },
      "outputs": [],
      "source": [
        "mostra_memoria()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koXWpJLHDhMf"
      },
      "outputs": [],
      "source": [
        "def preprocessa_queries(topics: dict) -> dict:\n",
        "    \"\"\"\n",
        "    Função que pré-processa o texto dos títulos dos tópicos do dataset TREC.\n",
        "\n",
        "    Args:\n",
        "        topics (dict): Dicionário com as queries do dataset TREC.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dicionário com as queries pré-processadas e seus respectivos tokens.\n",
        "    \"\"\"\n",
        "    topics_prep = {}\n",
        "\n",
        "    # Itera sobre as queries do dicionário topics\n",
        "    for query_id, query_text in tqdm(topics.items(), desc=\"Preprocessando queries\"):\n",
        "        # Realiza o pré-processamento do texto do título\n",
        "        title_prep = preprocessar(query_text['title'])\n",
        "        \n",
        "        # Adiciona o texto pré-processado na chave 'tokens'\n",
        "        query_text['tokens'] = title_prep\n",
        "        \n",
        "        # Adiciona o resultado ao novo dicionário\n",
        "        topics_prep[query_id] = query_text\n",
        "    \n",
        "    return topics_prep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kFbEa9VDpYu",
        "outputId": "a33626a4-cf98-4a73-cef4-c50996d3d04d"
      },
      "outputs": [],
      "source": [
        "topics_com_relevancia_prep = preprocessa_queries(topics_com_relevancia)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vrnz_pmnEpic",
        "outputId": "ab8a0321-141c-4653-8bf5-9aaef09f5ffd"
      },
      "outputs": [],
      "source": [
        "len(topics_com_relevancia_prep), list(topics_com_relevancia_prep.items())[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyPotpnAMCIr"
      },
      "source": [
        "## Desenvolvimento dos buscadores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fY4LVQkbMGV6"
      },
      "source": [
        "### Criando massa fictícia para testar código"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14Aktjyr0NAk"
      },
      "outputs": [],
      "source": [
        "# Inicializando queries preprocessadas\n",
        "\n",
        "# Inicializando documentos preprocessados\n",
        "documentos_prep_test = {\n",
        "    1: {'title': 'Document about international organized crime.', 'tokens': ['document', 'international', 'organized', 'crime']},\n",
        "    2: {'title': 'Media violence can increase aggressive behavior.', 'tokens': ['media', 'violence', 'increase', 'aggressive', 'behavior']},\n",
        "    3: {'title': 'Effective water management strategies.', 'tokens': ['effective', 'water', 'management', 'strategies']},\n",
        "    4: {'title': 'Report on transnational crime.', 'tokens': ['report', 'transnational', 'crime']},\n",
        "    5: {'title': 'The role of mass media in shaping public opinion.', 'tokens': ['role', 'mass', 'media', 'shaping', 'public', 'opinion']},\n",
        "    6: {'title': 'Water scarcity and conflicts.', 'tokens': ['water', 'scarcity', 'conflicts']},\n",
        "    7: {'title': 'Organized crime in Latin America.', 'tokens': ['organized', 'crime', 'latin', 'america']},\n",
        "    8: {'title': 'Impact of violent media on youth.', 'tokens': ['impact', 'violent', 'media', 'youth']},\n",
        "    9: {'title': 'Water resources in the Middle East.', 'tokens': ['water', 'resources', 'middle', 'east']},\n",
        "    10: {'title': 'Overview of organized crime.', 'tokens': ['overview', 'organized', 'crime']}\n",
        "}\n",
        "\n",
        "\n",
        "topics_prep_test = {\n",
        "    301: {'title': 'International Organized Crime', 'tokens': ['international', 'organized', 'crime']},\n",
        "    302: {'title': 'Mass Media and Violence', 'tokens': ['mass', 'media' ,'violence']},\n",
        "    303: {'title': 'Water Management', 'tokens': ['water', 'management']}\n",
        "}\n",
        "\n",
        "\n",
        "# Inicializando qrels\n",
        "qrels_test: Dict[str, Dict[int, int]] = {\n",
        "    301: {1: 3, 2: 2, 3: 0, 4: 1, 5: 0, 6: 0, 7: 1, 8: 0, 9: 0, 10: 1},\n",
        "    302: {1: 0, 2: 3, 3: 0, 4: 0, 5: 1, 6: 0, 7: 0, 8: 1, 9: 0, 10: 0},\n",
        "    303: {1: 0, 2: 0, 3: 1, 4: 0, 5: 0, 6: 3, 7: 0, 8: 0, 9: 2, 10: 0}\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWiudEnDM-Ct"
      },
      "source": [
        "### BooleanSearcher\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0dS4E8soSzKq"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwwUyxWKn8c6"
      },
      "outputs": [],
      "source": [
        "class BagofWordsSearcher:\n",
        "    \"\"\"\n",
        "    Classe responsável por criar um índice invertido de tokens de um conjunto de documentos\n",
        "    e realizar busca baseada na similaridade entre o índice e uma consulta de busca.\n",
        "\n",
        "    Parâmetros\n",
        "    ----------\n",
        "    docs : dict\n",
        "        Um dicionário onde as chaves são identificadores únicos para cada documento e os valores\n",
        "        são outros dicionários contendo informações sobre os documentos, como tokens e outras\n",
        "        informações relevantes.\n",
        "    device : torch.device, opcional\n",
        "        O dispositivo (CPU ou GPU) em que o índice invertido e os tensores relacionados serão alocados.\n",
        "        O valor padrão é \"cuda\" se o PyTorch detectar que uma GPU está disponível e \"cpu\" caso contrário.\n",
        "    parm_se_imprime : bool, opcional\n",
        "        Um parâmetro para controlar se mensagens de depuração devem ser impressas durante a execução\n",
        "        da classe. O valor padrão é False, ou seja, as mensagens não serão impressas.\n",
        "\n",
        "    Atributos\n",
        "    ---------\n",
        "    vocab : list\n",
        "        Uma lista contendo todos os tokens únicos encontrados em todos os documentos.\n",
        "    docs : dict\n",
        "        O mesmo dicionário passado como entrada no construtor.\n",
        "    _device : torch.device\n",
        "        O dispositivo em que o índice invertido e os tensores relacionados serão alocados.\n",
        "    _doc_ids : list\n",
        "        Uma lista de identificadores únicos de documentos, na mesma ordem que o tensor \"index\".\n",
        "    _tamanho_vocab : int\n",
        "        O número total de tokens únicos encontrados em todos os documentos.\n",
        "    index : torch.Tensor\n",
        "        Um tensor de tamanho (num_docs, num_tokens), onde cada linha representa um documento e cada\n",
        "        coluna representa um token, indicando quantas vezes o token aparece no documento.\n",
        "\n",
        "    Métodos\n",
        "    -------\n",
        "    _create_index()\n",
        "        Cria o índice invertido e armazena os resultados em \"vocab\", \"_doc_ids\", \"_tamanho_vocab\" e\n",
        "        \"index\".\n",
        "    _numericaliza(tokens)\n",
        "        Converte uma lista de tokens em um tensor representando a contagem de ocorrências de cada\n",
        "        token na lista, em relação ao vocabulário geral.\n",
        "    search(query)\n",
        "        Realiza uma busca baseada na similaridade entre o tensor de contagem de tokens da consulta e\n",
        "        o tensor de contagem de tokens de todos os documentos. Retorna uma lista de tuplas contendo\n",
        "        o identificador de cada documento e a medida de similaridade entre a consulta e o documento,\n",
        "        em ordem decrescente de similaridade.\n",
        "\n",
        "    Exemplos\n",
        "    --------\n",
        "    >>> docs = {\n",
        "    ...     \"doc1\": {\"tokens\": [\"foo\", \"bar\", \"baz\"]},\n",
        "    ...     \"doc2\": {\"tokens\": [\"foo\", \"foo\", \"bar\", \"qux\"]},\n",
        "    ...     \"doc3\": {\"tokens\": [\"baz\", \"qux\", \"quux\"]}\n",
        "    ... }\n",
        "    >>> bws = BagofWordsSearcher(docs)\n",
        "    >>> bws.search([\"foo\", \"bar\"])\n",
        "    [(\"doc2\", 2.0), (\"doc1\", 1.0), (\"doc3\", 0.0)]\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, docs, device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'), parm_se_imprime:bool=False):\n",
        "        \"\"\"\n",
        "        Construtor da classe BagofWordsSearcher.\n",
        "\n",
        "        Args:\n",
        "            docs (dict): Um dicionário contendo os documentos a serem indexados.\n",
        "            device (torch.device, optional): Dispositivo onde o índice será armazenado (GPU ou CPU).\n",
        "                                              O padrão é 'cuda' se uma GPU estiver disponível, caso contrário 'cpu'.\n",
        "            parm_se_imprime (bool, optional): Indica se informações de depuração devem ser impressas durante a execução.\n",
        "                                              O padrão é True.\n",
        "\n",
        "        Attributes:\n",
        "            _se_imprime (bool): Indica se informações de depuração devem ser impressas durante a execução.\n",
        "            vocab (list): Lista de palavras únicas encontradas nos documentos.\n",
        "            docs (dict): Dicionário contendo os documentos a serem indexados.\n",
        "            _device (torch.device): Dispositivo onde o índice será armazenado (GPU ou CPU).\n",
        "            _doc_ids (list): Lista com os IDs dos documentos.\n",
        "            _tamanho_vocab (int): Quantidade de palavras únicas encontradas nos documentos.\n",
        "            index (torch.Tensor): Matriz onde cada linha representa um documento e cada coluna representa a contagem\n",
        "                                  de uma palavra única.\n",
        "        \"\"\"\n",
        "        self._se_imprime = parm_se_imprime\n",
        "        self.vocab = None\n",
        "        self.docs = docs\n",
        "        self._device = device\n",
        "\n",
        "        # Imprime informações de depuração, se necessário\n",
        "        if self._se_imprime:\n",
        "            print(f\"Em __init__: self._device = {self._device}\")\n",
        "            print(f\"Em __init__: len(self.docs) = {len(self.docs)}\")\n",
        "\n",
        "        # Cria o índice invertido que representa todos os documentos da classe em um espaço vetorial.\n",
        "        self._create_index()\n",
        "\n",
        "    def _create_index(self):\n",
        "        \"\"\"\n",
        "        Cria o índice invertido que representa todos os documentos da classe em um espaço vetorial.\n",
        "\n",
        "        Cada documento é convertido em um vetor de tokens e, em seguida, um vocabulário é criado a partir de todos os\n",
        "        tokens de todos os documentos, sem repetições. A lista de documentos é transformada em uma matriz, onde cada\n",
        "        linha representa um documento e cada coluna representa um token do vocabulário. Cada posição da matriz representa\n",
        "        a frequência de um token em um documento.\n",
        "\n",
        "        Essa matriz é criada no dispositivo definido em self._device.\n",
        "\n",
        "        \"\"\"\n",
        "        # cria o conjunto de vocabulário que representa todos os tokens de todos os documentos, sem repetições\n",
        "        vocab = set()\n",
        "\n",
        "        # cria uma lista que vai conter o id de cada documento\n",
        "        doc_ids = []\n",
        "\n",
        "        # itera por todos os documentos e atualiza vocab com os tokens de cada documento, e doc_ids com o id do documento\n",
        "        for doc_id, doc in self.docs.items():\n",
        "\n",
        "            if type(doc) == dict:\n",
        "                vocab.update(set(doc['tokens']))\n",
        "            else: # type(doc) == list\n",
        "                vocab.update(set(doc))\n",
        "\n",
        "\n",
        "            doc_ids.append(doc_id)\n",
        "       \n",
        "\n",
        "        self.tipo_origem = type(self.docs[doc_ids[0]])\n",
        "\n",
        "        # transforma o conjunto vocab em uma lista, para preservar a ordem dos tokens\n",
        "        self.vocab = list(vocab)\n",
        "\n",
        "        # salva a lista de ids dos documentos\n",
        "        self._doc_ids = doc_ids\n",
        "\n",
        "        # salva o tamanho do vocabulário\n",
        "        self._tamanho_vocab = len(self.vocab)\n",
        "\n",
        "        # cria a matriz index, onde cada linha representa um documento e cada coluna representa um token do vocabulário\n",
        "        # a posição (i,j) da matriz representa a frequência do token j no documento i\n",
        "        # a matriz é criada no dispositivo definido em self._device\n",
        "        if self.tipo_origem == dict:\n",
        "            self.index = torch.stack([self._numericaliza(doc[\"tokens\"]) for doc in self.docs.values()]).to(self._device)\n",
        "        elif self.tipo_origem == list:\n",
        "            self.index = torch.stack([self._numericaliza(doc) for doc in self.docs.values()]).to(self._device)\n",
        "\n",
        "        if self._se_imprime:\n",
        "            print(f\"Em _create_index: self.vocab = {self.vocab}\")        \n",
        "            print(f\"Em _create_index: self._doc_ids = {self._doc_ids}\")        \n",
        "            print(f\"Em _create_index: self._tamanho_vocab = {self._tamanho_vocab}\")          \n",
        "            print(f\"Em _create_index: self.index = {self.index}\")          \n",
        "            print(f\"Em _create_index: self.tipo_origem = {self.tipo_origem}\")          \n",
        "\n",
        "    def _numericaliza(self, tokens):\n",
        "        \"\"\"\n",
        "        Transforma uma lista de tokens em um tensor com a contagem de ocorrências de cada token na lista.\n",
        "\n",
        "        Args:\n",
        "            tokens (list): lista de tokens.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: tensor com a contagem de ocorrências de cada token na lista.\n",
        "        \"\"\"\n",
        "        # Cria um objeto Counter com a contagem de ocorrências de cada token na lista\n",
        "        token_counts = Counter(tokens)\n",
        "        \n",
        "        # Obtém os índices de cada token na lista de vocabulário (se existir)\n",
        "        indexes = [self.vocab.index(token) for token in token_counts.keys() if token in self.vocab]\n",
        "        \n",
        "        # Cria um tensor com zeros, com o mesmo tamanho do vocabulário\n",
        "        values = torch.zeros(self._tamanho_vocab, device=self._device)\n",
        "        \n",
        "        # Para cada token na contagem, atualiza o tensor values na posição correspondente ao índice do token\n",
        "        for token, count in token_counts.items():\n",
        "            if token in self.vocab:\n",
        "                values[self.vocab.index(token)] = count\n",
        "\n",
        "        if self._se_imprime:\n",
        "            print(f\"Em _numericaliza: token_counts = {token_counts}\")\n",
        "            print(f\"Em _numericaliza: indexes = {indexes}\")\n",
        "            print(f\"Em _numericaliza: values = {values}\")\n",
        "\n",
        "        return values\n",
        "\n",
        "\n",
        "    def search(self, query: list, k:int=10):\n",
        "        \"\"\"\n",
        "        Realiza uma busca por similaridade entre o documento e a query fornecidos. Retorna uma lista de tuplas\n",
        "        contendo o id do documento e sua similaridade com a query, ordenada de forma decrescente pela similaridade.\n",
        "\n",
        "        Parâmetros:\n",
        "        -----------\n",
        "        query : list\n",
        "            Lista de tokens da query.\n",
        "\n",
        "        Retorno:\n",
        "        --------\n",
        "        relevant_docs : list\n",
        "            Lista de tuplas (id do documento, similaridade) ordenada de forma decrescente pela similaridade.\n",
        "        \"\"\"\n",
        "        # Converte a query em um tensor numérico.\n",
        "        query_tensor = self._numericaliza(query).unsqueeze(0).to(self._device)\n",
        "                    \n",
        "        # Calcula a similaridade entre a query e todos os documentos da base de dados.\n",
        "        similarities = torch.matmul(query_tensor, self.index.T).squeeze(dim=0)\n",
        "                \n",
        "        # Gera uma lista de tuplas contendo o id do documento e sua similaridade com a query.\n",
        "        result = [(self._doc_ids[i], s) for i, s in enumerate(similarities.tolist())]\n",
        "                    \n",
        "        # Ordena a lista de documentos relevantes pela similaridade em ordem decrescente.\n",
        "        relevant_docs = sorted(result, key=lambda x: x[1], reverse=True)[:k]\n",
        "\n",
        "        if self._se_imprime:\n",
        "            print(f\"Em search: query_tensor = {query_tensor}\")\n",
        "            print(f\"Em search: similarities = {similarities}\")\n",
        "            print(f\"Em search: result = {result}\")\n",
        "            print(f\"Em search: relevant_docs = {relevant_docs}\")                    \n",
        "        return relevant_docs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rxoG-ddNS6w",
        "outputId": "c7065350-36e8-41e3-c0b3-a26f479f721b"
      },
      "outputs": [],
      "source": [
        "bow_searcher = BagofWordsSearcher(documentos_prep_test, parm_se_imprime=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0PYbDS8NVZ3",
        "outputId": "bca57047-2c68-4c8f-f14a-9bf04c5a6b0d"
      },
      "outputs": [],
      "source": [
        "bow_searcher.tipo_origem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10JINP1P9MnG",
        "outputId": "75852ca3-596e-4e49-8355-c55e834caae9"
      },
      "outputs": [],
      "source": [
        "bow_searcher.search(topics_prep_test[301]['tokens'],k=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JMFBtxLs3yj"
      },
      "source": [
        "## Calculando a métrica ndcg@10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUgslhQp8N4m"
      },
      "outputs": [],
      "source": [
        "bow_searcher = BagofWordsSearcher(documentos_prep_test, parm_se_imprime=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnMB-u5IM-5B"
      },
      "outputs": [],
      "source": [
        "def ndcg_at_k(ranking_docto, relevance_ordenada, dict_relevancia, k=10):\n",
        "    dcg = 0.0\n",
        "    idcg = 0.0\n",
        "    for i, docto_id in enumerate(ranking_docto):\n",
        "        if i > k:\n",
        "            break\n",
        "        dcg += dict_relevancia[docto_id] / torch.log2(torch.tensor(i + 2))\n",
        "        idcg += dict_relevancia[relevance_ordenada[i]] / torch.log2(torch.tensor(i + 2))\n",
        "        print(f'i={i}, docto_id={docto_id} dcg={dcg} idcg={idcg}')\n",
        "\n",
        "    val_metric = dcg / idcg\n",
        "    print(f\"val_metric = dcg / idcg :: {val_metric} = {dcg} / {idcg}  \")\n",
        "    return dcg / idcg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TipLkig8AKoL"
      },
      "outputs": [],
      "source": [
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FuaMrp5-AAL1"
      },
      "outputs": [],
      "source": [
        "def ndcg_at_k_query(ranking_docto, relevance_ordenada, dict_relevancia, k=10):\n",
        "    \"\"\"\n",
        "    Calcula a métrica NDCG@k.\n",
        "\n",
        "    Args:\n",
        "        ranking_docto (list): Lista com o ID dos documentos retornados pela busca.\n",
        "        relevance_ordenada (list): Lista com o ID dos documentos relevantes para a consulta, \n",
        "                                   ordenados pela relevância.\n",
        "        dict_relevancia (dict): Dicionário que mapeia o ID do documento à sua relevância.\n",
        "        k (int): Número de documentos considerados na métrica.\n",
        "\n",
        "    Returns:\n",
        "        float: O valor da métrica NDCG@k para a consulta.\n",
        "\n",
        "    \"\"\"\n",
        "    dcg = 0.0  # inicializa o valor de dcg como 0\n",
        "    idcg = 0.0  # inicializa o valor de idcg como 0\n",
        "\n",
        "    # percorre o ranking de documentos\n",
        "    for i, docto_id in enumerate(ranking_docto):\n",
        "        if i >= k:\n",
        "            break  # para de processar documentos se já chegou no número k\n",
        "\n",
        "        # calcula o valor de dcg para o documento atual\n",
        "        rel = dict_relevancia[docto_id]  # relevância do documento\n",
        "\n",
        "        # calcula o valor de idcg para o documento atual\n",
        "        rel_idcg = dict_relevancia[relevance_ordenada[i]]  # relevância do documento considerado ideal\n",
        "\n",
        "        # acumula \n",
        "        if rel > 0:\n",
        "            dcg += (2 ** rel - 1) / math.log2(i + 2)\n",
        "        if rel_idcg > 0:\n",
        "            idcg += (2 ** rel_idcg - 1) / math.log2(i + 2)\n",
        "\n",
        "        # imprime as informações para depuração\n",
        "        print(f'i={i}, docto_id={docto_id}  rel={rel} rel_idcg={rel_idcg} dcg={round(dcg,2)} idcg={round(idcg,2)}')\n",
        "\n",
        "    # calcula o valor final da métrica\n",
        "    val_metric = dcg / idcg if idcg > 0 else 0.0\n",
        "    print(f\"val_metric = dcg / idcg :: {val_metric} = {dcg} / {idcg}  \")\n",
        "    return round(val_metric,2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tWkVHUfEvc-"
      },
      "outputs": [],
      "source": [
        "def calcula_ndcg_at_k(topics, qrels, searcher, k, se_imprime:bool=False):\n",
        "    ndcg_scores = []\n",
        "    for query_id, query in topics.items():\n",
        "        # Realizando a busca\n",
        "        results = bow_searcher.search(query['tokens'], k=k)\n",
        "\n",
        "        # obtém as relevâncias para a query atual\n",
        "        dict_relevancia = qrels[query_id]\n",
        "        relevances = [id_docto for id_docto, relevance in sorted(dict_relevancia.items(), key=lambda x: x[1], reverse=True)]\n",
        "\n",
        "        # Obtendo o ranking com o id dos documentos retornados\n",
        "        ranking = [par_docid_relevance[0] for par_docid_relevance in results]\n",
        "\n",
        "        # Calculando a métrica ndcg@10\n",
        "        ndcg_score = ndcg_at_k_query(ranking, relevances, dict_relevancia, k=10)\n",
        "        if se_imprime:\n",
        "            print(f\"no cálculo da métrica: query_id={query_id}, query['tokens']={query['tokens']}\")\n",
        "            print(f'no cálculo da métrica: results = {results}')\n",
        "            print(f'no cálculo da métrica: dict_relevancia = {dict_relevancia}')    \n",
        "            print(f'no cálculo da métrica: relevances = {relevances}')\n",
        "            print(f'no cálculo da métrica: ranking = {ranking}')\n",
        "            print(f'no cálculo da métrica: ndcg_score = {ndcg_score}')  \n",
        "        # Armazenando a métrica para a query atual\n",
        "        ndcg_scores.append((query_id, ndcg_score))\n",
        "        \n",
        "    # Calculando a média dos ndcg\n",
        "    ndcg_mean = sum([score[1] for score in ndcg_scores])/len(ndcg_scores)\n",
        "    return ndcg_mean, ndcg_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrxMLZWKE-i2"
      },
      "outputs": [],
      "source": [
        "ndcg_mean, ndcg_scores = calcula_ndcg_at_k(topics_prep_test, qrels_test, bow_searcher, k=10)\n",
        "print(f\"ndcg_mean: {ndcg_mean}\")\n",
        "print(f\"ndcg_scores: {ndcg_scores}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-ePdFPSVKIu"
      },
      "outputs": [],
      "source": [
        "class BooleanSearcher:\n",
        "    \"\"\"\n",
        "    Classe responsável por criar um índice invertido de tokens de um conjunto de documentos\n",
        "    e realizar busca baseada na similaridade entre o índice e uma consulta de busca.\n",
        "\n",
        "    Parâmetros\n",
        "    ----------\n",
        "    docs : dict\n",
        "        Um dicionário onde as chaves são identificadores únicos para cada documento e os valores\n",
        "        são outros dicionários contendo informações sobre os documentos, como tokens e outras\n",
        "        informações relevantes.\n",
        "    device : torch.device, opcional\n",
        "        O dispositivo (CPU ou GPU) em que o índice invertido e os tensores relacionados serão alocados.\n",
        "        O valor padrão é \"cuda\" se o PyTorch detectar que uma GPU está disponível e \"cpu\" caso contrário.\n",
        "    parm_se_imprime : bool, opcional\n",
        "        Um parâmetro para controlar se mensagens de depuração devem ser impressas durante a execução\n",
        "        da classe. O valor padrão é False, ou seja, as mensagens não serão impressas.\n",
        "\n",
        "    Atributos\n",
        "    ---------\n",
        "    vocab : list\n",
        "        Uma lista contendo todos os tokens únicos encontrados em todos os documentos.\n",
        "    docs : dict\n",
        "        O mesmo dicionário passado como entrada no construtor.\n",
        "    _device : torch.device\n",
        "        O dispositivo em que o índice invertido e os tensores relacionados serão alocados.\n",
        "    _doc_ids : list\n",
        "        Uma lista de identificadores únicos de documentos, na mesma ordem que o tensor \"index\".\n",
        "    _tamanho_vocab : int\n",
        "        O número total de tokens únicos encontrados em todos os documentos.\n",
        "    index : torch.Tensor\n",
        "        Um tensor de tamanho (num_docs, num_tokens), onde cada linha representa um documento e cada\n",
        "        coluna representa um token, indicando quantas vezes o token aparece no documento.\n",
        "\n",
        "    Métodos\n",
        "    -------\n",
        "    _create_index()\n",
        "        Cria o índice invertido e armazena os resultados em \"vocab\", \"_doc_ids\", \"_tamanho_vocab\" e\n",
        "        \"index\".\n",
        "    _numericaliza(tokens)\n",
        "        Converte uma lista de tokens em um tensor representando a contagem de ocorrências de cada\n",
        "        token na lista, em relação ao vocabulário geral.\n",
        "    search(query)\n",
        "        Realiza uma busca baseada na similaridade entre o tensor de contagem de tokens da consulta e\n",
        "        o tensor de contagem de tokens de todos os documentos. Retorna uma lista de tuplas contendo\n",
        "        o identificador de cada documento e a medida de similaridade entre a consulta e o documento,\n",
        "        em ordem decrescente de similaridade.\n",
        "\n",
        "    Exemplos\n",
        "    --------\n",
        "    >>> docs = {\n",
        "    ...     \"doc1\": {\"tokens\": [\"foo\", \"bar\", \"baz\"]},\n",
        "    ...     \"doc2\": {\"tokens\": [\"foo\", \"foo\", \"bar\", \"qux\"]},\n",
        "    ...     \"doc3\": {\"tokens\": [\"baz\", \"qux\", \"quux\"]}\n",
        "    ... }\n",
        "    >>> bws = BagofWordsSearcher(docs)\n",
        "    >>> bws.search([\"foo\", \"bar\"])\n",
        "    [(\"doc2\", 2.0), (\"doc1\", 1.0), (\"doc3\", 0.0)]\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, docs, device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'), parm_se_imprime:bool=False):\n",
        "        \"\"\"\n",
        "        Construtor da classe BagofWordsSearcher.\n",
        "\n",
        "        Args:\n",
        "            docs (dict): Um dicionário contendo os documentos a serem indexados.\n",
        "            device (torch.device, optional): Dispositivo onde o índice será armazenado (GPU ou CPU).\n",
        "                                              O padrão é 'cuda' se uma GPU estiver disponível, caso contrário 'cpu'.\n",
        "            parm_se_imprime (bool, optional): Indica se informações de depuração devem ser impressas durante a execução.\n",
        "                                              O padrão é True.\n",
        "\n",
        "        Attributes:\n",
        "            _se_imprime (bool): Indica se informações de depuração devem ser impressas durante a execução.\n",
        "            vocab (list): Lista de palavras únicas encontradas nos documentos.\n",
        "            docs (dict): Dicionário contendo os documentos a serem indexados.\n",
        "            _device (torch.device): Dispositivo onde o índice será armazenado (GPU ou CPU).\n",
        "            _doc_ids (list): Lista com os IDs dos documentos.\n",
        "            _tamanho_vocab (int): Quantidade de palavras únicas encontradas nos documentos.\n",
        "            index (torch.Tensor): Matriz onde cada linha representa um documento e cada coluna representa a contagem\n",
        "                                  de uma palavra única.\n",
        "        \"\"\"\n",
        "        self._se_imprime = parm_se_imprime\n",
        "        self.vocab = None\n",
        "        self.docs = docs\n",
        "        self._device = device\n",
        "\n",
        "        # Imprime informações de depuração, se necessário\n",
        "        if self._se_imprime:\n",
        "            print(f\"Em __init__: self._device = {self._device}\")\n",
        "            print(f\"Em __init__: len(self.docs) = {len(self.docs)}\")\n",
        "\n",
        "        # Cria o índice invertido que representa todos os documentos da classe em um espaço vetorial.\n",
        "        self._create_index()\n",
        "\n",
        "    def _create_index(self):\n",
        "        \"\"\"\n",
        "        Cria o índice invertido que representa todos os documentos da classe em um espaço vetorial.\n",
        "\n",
        "        Cada documento é convertido em um vetor de tokens e, em seguida, um vocabulário é criado a partir de todos os\n",
        "        tokens de todos os documentos, sem repetições. A lista de documentos é transformada em uma matriz, onde cada\n",
        "        linha representa um documento e cada coluna representa um token do vocabulário. Cada posição da matriz representa\n",
        "        a frequência de um token em um documento.\n",
        "\n",
        "        Essa matriz é criada no dispositivo definido em self._device.\n",
        "\n",
        "        \"\"\"\n",
        "        # cria o conjunto de vocabulário que representa todos os tokens de todos os documentos, sem repetições\n",
        "        vocab = set()\n",
        "\n",
        "        # cria uma lista que vai conter o id de cada documento\n",
        "        doc_ids = []\n",
        "\n",
        "        # itera por todos os documentos e atualiza vocab com os tokens de cada documento, e doc_ids com o id do documento\n",
        "        for doc_id, doc in self.docs.items():\n",
        "\n",
        "            if type(doc) == dict:\n",
        "                vocab.update(set(doc['tokens']))\n",
        "            else: # type(doc) == list\n",
        "                vocab.update(set(doc))\n",
        "\n",
        "\n",
        "            doc_ids.append(doc_id)\n",
        "       \n",
        "\n",
        "        self.tipo_origem = type(self.docs[doc_ids[0]])\n",
        "\n",
        "        # transforma o conjunto vocab em uma lista, para preservar a ordem dos tokens\n",
        "        self.vocab = list(vocab)\n",
        "\n",
        "        # salva a lista de ids dos documentos\n",
        "        self._doc_ids = doc_ids\n",
        "\n",
        "        # salva o tamanho do vocabulário\n",
        "        self._tamanho_vocab = len(self.vocab)\n",
        "\n",
        "        # cria a matriz index, onde cada linha representa um documento e cada coluna representa um token do vocabulário\n",
        "        # a posição (i,j) da matriz representa a frequência do token j no documento i\n",
        "        # a matriz é criada no dispositivo definido em self._device\n",
        "        if self.tipo_origem == dict:\n",
        "            self.index = torch.stack([self._numericaliza(doc[\"tokens\"]) for doc in self.docs.values()]).to(self._device)\n",
        "        elif self.tipo_origem == list:\n",
        "            self.index = torch.stack([self._numericaliza(doc) for doc in self.docs.values()]).to(self._device)\n",
        "\n",
        "        if self._se_imprime:\n",
        "            print(f\"Em _create_index: self.vocab = {self.vocab}\")        \n",
        "            print(f\"Em _create_index: self._doc_ids = {self._doc_ids}\")        \n",
        "            print(f\"Em _create_index: self._tamanho_vocab = {self._tamanho_vocab}\")          \n",
        "            print(f\"Em _create_index: self.index = {self.index}\")          \n",
        "            print(f\"Em _create_index: self.tipo_origem = {self.tipo_origem}\")          \n",
        "\n",
        "    def _numericaliza(self, tokens):\n",
        "        \"\"\"\n",
        "        Transforma uma lista de tokens em um tensor com a indicação se ocorre (sim ou não) cada token na lista.\n",
        "\n",
        "        Args:\n",
        "            tokens (list): lista de tokens.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: tensor com a indicação se ocorre (sim ou não) cada token na lista.\n",
        "        \"\"\"\n",
        "        # Cria um objeto Counter com a contagem de ocorrências de cada token na lista\n",
        "        token_counts = Counter(tokens)\n",
        "        \n",
        "        # Obtém os índices de cada token na lista de vocabulário (se existir)\n",
        "        indexes = [self.vocab.index(token) for token in token_counts.keys() if token in self.vocab]\n",
        "        \n",
        "        # Cria um tensor com zeros, com o mesmo tamanho do vocabulário\n",
        "        values = torch.zeros(self._tamanho_vocab, device=self._device)\n",
        "        \n",
        "        # Para cada token na contagem, atualiza o tensor values na posição correspondente ao índice do token\n",
        "        for token, count in token_counts.items():\n",
        "            if token in self.vocab:\n",
        "                values[self.vocab.index(token)] = 1\n",
        "\n",
        "        if self._se_imprime:\n",
        "            print(f\"Em _numericaliza: token_counts = {token_counts}\")\n",
        "            print(f\"Em _numericaliza: indexes = {indexes}\")\n",
        "            print(f\"Em _numericaliza: values = {values}\")\n",
        "\n",
        "        return values\n",
        "\n",
        "\n",
        "    def search(self, query: list, k:int=10):\n",
        "        \"\"\"\n",
        "        Realiza uma busca por similaridade entre o documento e a query fornecidos. Retorna uma lista de tuplas\n",
        "        contendo o id do documento e sua similaridade com a query, ordenada de forma decrescente pela similaridade.\n",
        "\n",
        "        Parâmetros:\n",
        "        -----------\n",
        "        query : list\n",
        "            Lista de tokens da query.\n",
        "\n",
        "        Retorno:\n",
        "        --------\n",
        "        relevant_docs : list\n",
        "            Lista de tuplas (id do documento, similaridade) ordenada de forma decrescente pela similaridade.\n",
        "        \"\"\"\n",
        "        # Converte a query em um tensor numérico.\n",
        "        query_tensor = self._numericaliza(query).unsqueeze(0).to(self._device)\n",
        "                    \n",
        "        # Calcula a similaridade entre a query e todos os documentos da base de dados.\n",
        "        similarities = torch.matmul(query_tensor, self.index.T).squeeze(dim=0)\n",
        "                \n",
        "        # Gera uma lista de tuplas contendo o id do documento e sua similaridade com a query.\n",
        "        result = [(self._doc_ids[i], s) for i, s in enumerate(similarities.tolist())]\n",
        "                    \n",
        "        # Ordena a lista de documentos relevantes pela similaridade em ordem decrescente.\n",
        "        relevant_docs = sorted(result, key=lambda x: x[1], reverse=True)[:k]\n",
        "\n",
        "        if self._se_imprime:\n",
        "            print(f\"Em search: query_tensor = {query_tensor}\")\n",
        "            print(f\"Em search: similarities = {similarities}\")\n",
        "            print(f\"Em search: result = {result}\")\n",
        "            print(f\"Em search: relevant_docs = {relevant_docs}\")                    \n",
        "        return relevant_docs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7vyhKyRGryg"
      },
      "source": [
        "### Criando searcher para trec2020"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gga_R2OPks3"
      },
      "source": [
        "Testando em um pedaço do doctos_trec2020_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDZhdO9eJlo7",
        "outputId": "2cc1bf1d-f7f3-4192-f477-eb6285e02714"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFJ4JclELYdZ"
      },
      "outputs": [],
      "source": [
        "parte_doctos_trec2020_dict = {key: value for key, value in list(doctos_trec2020_dict.items())[0:2]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1V5TV0XxPpC0"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JKSkqjeJqiz"
      },
      "outputs": [],
      "source": [
        "bow_searcher = BagofWordsSearcher(parte_doctos_trec2020_dict, parm_se_imprime=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jL31_eeQPY1e",
        "outputId": "4e801804-2dc5-4d98-e54d-7b04ba11ba4e"
      },
      "outputs": [],
      "source": [
        "topics_com_relevancia_prep[23849]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQbMycjXvAQJ",
        "outputId": "3559b597-6bde-4be0-eee6-20898dae692d"
      },
      "outputs": [],
      "source": [
        "bow_searcher.search(topics_com_relevancia_prep[23849])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcjObbRrPsvV"
      },
      "source": [
        "Criando para todo o doctos_trec2020_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXc6_kYfP1ww",
        "outputId": "76d0a5b9-e022-489c-a165-fb71cfb3d7a3"
      },
      "outputs": [],
      "source": [
        "mostra_memoria()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQ_GWWl-PW5A"
      },
      "outputs": [],
      "source": [
        "bow_searcher = BagofWordsSearcher(doctos_trec2020_dict, parm_se_imprime=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yto2EOAqP37N"
      },
      "outputs": [],
      "source": [
        "mostra_memoria()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgspVbxmP7TN"
      },
      "outputs": [],
      "source": [
        "ndcg_mean, ndcg_scores = calcula_ndcg_at_k(topics_com_relevancia_prep, qrels, bow_searcher, k=10)\n",
        "print(f\"ndcg_mean: {ndcg_mean}\")\n",
        "print(f\"ndcg_scores: {ndcg_scores}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "haoN-F06wryO",
        "emUYIElNoHmP",
        "sQWG3iK-wbHn",
        "S6zEVa4Hv2bv",
        "_fCVAuEgMaYE",
        "c-EQ6_F2VktR",
        "8y_HkjXlPTuL",
        "waqMpGVHPVoi"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
